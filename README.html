<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>README</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style type="text/css">
      body {
          -ms-text-size-adjust: 100%;
          -webkit-text-size-adjust: 100%;
          line-height: 1.5;
          color: #333;
          font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
          font-size: 16px;
          line-height: 1.5;
          word-wrap: break-word;
      }
      
      .pl-c {
          color: #969896;
      }
      
      .pl-c1,
      .pl-s .pl-v {
          color: #0086b3;
      }
      
      .pl-e,
      .pl-en {
          color: #795da3;
      }
      
      .pl-smi,
      .pl-s .pl-s1 {
          color: #333;
      }
      
      .pl-ent {
          color: #63a35c;
      }
      
      .pl-k {
          color: #a71d5d;
      }
      
      .pl-s,
      .pl-pds,
      .pl-s .pl-pse .pl-s1,
      .pl-sr,
      .pl-sr .pl-cce,
      .pl-sr .pl-sre,
      .pl-sr .pl-sra {
          color: #183691;
      }
      
      .pl-v {
          color: #ed6a43;
      }
      
      .pl-id {
          color: #b52a1d;
      }
      
      .pl-ii {
          color: #f8f8f8;
          background-color: #b52a1d;
      }
      
      .pl-sr .pl-cce {
          font-weight: bold;
          color: #63a35c;
      }
      
      .pl-ml {
          color: #693a17;
      }
      
      .pl-mh,
      .pl-mh .pl-en,
      .pl-ms {
          font-weight: bold;
          color: #1d3e81;
      }
      
      .pl-mq {
          color: #008080;
      }
      
      .pl-mi {
          font-style: italic;
          color: #333;
      }
      
      .pl-mb {
          font-weight: bold;
          color: #333;
      }
      
      .pl-md {
          color: #bd2c00;
          background-color: #ffecec;
      }
      
      .pl-mi1 {
          color: #55a532;
          background-color: #eaffea;
      }
      
      .pl-mdr {
          font-weight: bold;
          color: #795da3;
      }
      
      .pl-mo {
          color: #1d3e81;
      }
      
      .octicon {
          display: inline-block;
          vertical-align: text-top;
          fill: currentColor;
      }
      
      a {
          background-color: transparent;
          -webkit-text-decoration-skip: objects;
      }
      
      a:active,
      a:hover {
          outline-width: 0;
      }
      
      strong {
          font-weight: inherit;
      }
      
      strong {
          font-weight: bolder;
      }
      
      h1 {
          font-size: 2em;
          margin: 0.67em 0;
      }
      
      img {
          border-style: none;
      }
      
      svg:not(:root) {
          overflow: hidden;
      }
      
      code,
      kbd,
      pre {
          font-family: monospace, monospace;
          font-size: 1em;
      }
      
      hr {
          box-sizing: content-box;
          height: 0;
          overflow: visible;
      }
      
      input {
          font: inherit;
          margin: 0;
      }
      
      input {
          overflow: visible;
      }
      
      [type="checkbox"] {
          box-sizing: border-box;
          padding: 0;
      }
      
      * {
          box-sizing: border-box;
      }
      
      input {
          font-family: inherit;
          font-size: inherit;
          line-height: inherit;
      }
      
      a {
          color: #4078c0;
          text-decoration: none;
      }
      
      a:hover,
      a:active {
          text-decoration: underline;
      }
      
      strong {
          font-weight: 600;
      }
      
      hr {
          height: 0;
          margin: 15px 0;
          overflow: hidden;
          background: transparent;
          border: 0;
          border-bottom: 1px solid #ddd;
      }
      
      hr::before {
          display: table;
          content: "";
      }
      
      hr::after {
          display: table;
          clear: both;
          content: "";
      }
      
      table {
          border-spacing: 0;
          border-collapse: collapse;
      }
      
      td,
      th {
          padding: 0;
      }
      
      h1,
      h2,
      h3,
      h4,
      h5,
      h6 {
          margin-top: 0;
          margin-bottom: 0;
      }
      
      h1 {
          font-size: 32px;
          font-weight: 600;
      }
      
      h2 {
          font-size: 24px;
          font-weight: 600;
      }
      
      h3 {
          font-size: 20px;
          font-weight: 600;
      }
      
      h4 {
          font-size: 16px;
          font-weight: 600;
      }
      
      h5 {
          font-size: 14px;
          font-weight: 600;
      }
      
      h6 {
          font-size: 12px;
          font-weight: 600;
      }
      
      p {
          margin-top: 0;
          margin-bottom: 10px;
      }
      
      blockquote {
          margin: 0;
      }
      
      ul,
      ol {
          padding-left: 0;
          margin-top: 0;
          margin-bottom: 0;
      }
      
      ol ol,
      ul ol {
          list-style-type: lower-roman;
      }
      
      ul ul ol,
      ul ol ol,
      ol ul ol,
      ol ol ol {
          list-style-type: lower-alpha;
      }
      
      dd {
          margin-left: 0;
      }
      
      code {
          font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
          font-size: 12px;
      }
      
      pre {
          margin-top: 0;
          margin-bottom: 0;
          font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
      }
      
      .octicon {
          vertical-align: text-bottom;
      }
      
      input {
          -webkit-font-feature-settings: "liga" 0;
          font-feature-settings: "liga" 0;
      }
      
      .markdown-body::before {
          display: table;
          content: "";
      }
      
      .markdown-body::after {
          display: table;
          clear: both;
          content: "";
      }
      
      .markdown-body>*:first-child {
          margin-top: 0 !important;
      }
      
      .markdown-body>*:last-child {
          margin-bottom: 0 !important;
      }
      
      a:not([href]) {
          color: inherit;
          text-decoration: none;
      }
      
      .anchor {
          float: left;
          padding-right: 4px;
          margin-left: -20px;
          line-height: 1;
      }
      
      .anchor:focus {
          outline: none;
      }
      
      p,
      blockquote,
      ul,
      ol,
      dl,
      table,
      pre {
          margin-top: 0;
          margin-bottom: 16px;
      }
      
      hr {
          height: 0.25em;
          padding: 0;
          margin: 24px 0;
          background-color: #e7e7e7;
          border: 0;
      }
      
      blockquote {
          padding: 0 1em;
          color: #777;
          border-left: 0.25em solid #ddd;
      }
      
      blockquote>:first-child {
          margin-top: 0;
      }
      
      blockquote>:last-child {
          margin-bottom: 0;
      }
      
      kbd {
          display: inline-block;
          padding: 3px 5px;
          font-size: 11px;
          line-height: 10px;
          color: #555;
          vertical-align: middle;
          background-color: #fcfcfc;
          border: solid 1px #ccc;
          border-bottom-color: #bbb;
          border-radius: 3px;
          box-shadow: inset 0 -1px 0 #bbb;
      }
      
      h1,
      h2,
      h3,
      h4,
      h5,
      h6 {
          margin-top: 24px;
          margin-bottom: 16px;
          font-weight: 600;
          line-height: 1.25;
      }
      
      h1 .octicon-link,
      h2 .octicon-link,
      h3 .octicon-link,
      h4 .octicon-link,
      h5 .octicon-link,
      h6 .octicon-link {
          color: #000;
          vertical-align: middle;
          visibility: hidden;
      }
      
      h1:hover .anchor,
      h2:hover .anchor,
      h3:hover .anchor,
      h4:hover .anchor,
      h5:hover .anchor,
      h6:hover .anchor {
          text-decoration: none;
      }
      
      h1:hover .anchor .octicon-link,
      h2:hover .anchor .octicon-link,
      h3:hover .anchor .octicon-link,
      h4:hover .anchor .octicon-link,
      h5:hover .anchor .octicon-link,
      h6:hover .anchor .octicon-link {
          visibility: visible;
      }
      
      h1 {
          padding-bottom: 0.3em;
          font-size: 2em;
          border-bottom: 1px solid #eee;
      }
      
      h2 {
          padding-bottom: 0.3em;
          font-size: 1.5em;
          border-bottom: 1px solid #eee;
      }
      
      h3 {
          font-size: 1.25em;
      }
      
      h4 {
          font-size: 1em;
      }
      
      h5 {
          font-size: 0.875em;
      }
      
      h6 {
          font-size: 0.85em;
          color: #777;
      }
      
      ul,
      ol {
          padding-left: 2em;
      }
      
      ul ul,
      ul ol,
      ol ol,
      ol ul {
          margin-top: 0;
          margin-bottom: 0;
      }
      
      li>p {
          margin-top: 16px;
      }
      
      li+li {
          margin-top: 0.25em;
      }
      
      dl {
          padding: 0;
      }
      
      dl dt {
          padding: 0;
          margin-top: 16px;
          font-size: 1em;
          font-style: italic;
          font-weight: bold;
      }
      
      dl dd {
          padding: 0 16px;
          margin-bottom: 16px;
      }
      
      table {
          display: block;
          width: 100%;
          overflow: auto;
      }
      
      table th {
          font-weight: bold;
      }
      
      table th,
      table td {
          padding: 6px 13px;
          border: 1px solid #ddd;
      }
      
      table tr {
          background-color: #fff;
          border-top: 1px solid #ccc;
      }
      
      table tr:nth-child(2n) {
          background-color: #f8f8f8;
      }
      
      img {
          max-width: 100%;
          box-sizing: content-box;
          background-color: #fff;
      }
      
      code {
          padding: 0;
          padding-top: 0.2em;
          padding-bottom: 0.2em;
          margin: 0;
          font-size: 85%;
          background-color: rgba(0, 0, 0, 0.04);
          border-radius: 3px;
      }
      
      code::before,
      code::after {
          letter-spacing: -0.2em;
          content: "\00a0";
      }
      
      pre {
          word-wrap: normal;
      }
      
      pre>code {
          padding: 0;
          margin: 0;
          font-size: 100%;
          word-break: normal;
          white-space: pre;
          background: transparent;
          border: 0;
      }
      
      .highlight {
          margin-bottom: 16px;
      }
      
      .highlight pre {
          margin-bottom: 0;
          word-break: normal;
      }
      
      .highlight pre,
      pre {
          padding: 16px;
          overflow: auto;
          font-size: 85%;
          line-height: 1.45;
          background-color: #f7f7f7;
          border-radius: 3px;
      }
      
      pre code {
          display: inline;
          max-width: auto;
          padding: 0;
          margin: 0;
          overflow: visible;
          line-height: inherit;
          word-wrap: normal;
          background-color: transparent;
          border: 0;
      }
      
      pre code::before,
      pre code::after {
          content: normal;
      }
      
      .pl-0 {
          padding-left: 0 !important;
      }
      
      .pl-1 {
          padding-left: 3px !important;
      }
      
      .pl-2 {
          padding-left: 6px !important;
      }
      
      .pl-3 {
          padding-left: 12px !important;
      }
      
      .pl-4 {
          padding-left: 24px !important;
      }
      
      .pl-5 {
          padding-left: 36px !important;
      }
      
      .pl-6 {
          padding-left: 48px !important;
      }
      
      .full-commit .btn-outline:not(:disabled):hover {
          color: #4078c0;
          border: 1px solid #4078c0;
      }
      
      kbd {
          display: inline-block;
          padding: 3px 5px;
          font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
          line-height: 10px;
          color: #555;
          vertical-align: middle;
          background-color: #fcfcfc;
          border: solid 1px #ccc;
          border-bottom-color: #bbb;
          border-radius: 3px;
          box-shadow: inset 0 -1px 0 #bbb;
      }
      
       :checked+.radio-label {
          position: relative;
          z-index: 1;
          border-color: #4078c0;
      }
      
      .task-list-item {
          list-style-type: none;
      }
      
      .task-list-item+.task-list-item {
          margin-top: 3px;
      }
      
      .task-list-item input {
          margin: 0 0.2em 0.25em -1.6em;
          vertical-align: middle;
      }
      
      hr {
          border-bottom-color: #eee;
      }
      /** Theming **/
      
      body {
          width: 890px;
          margin: 0 auto;
      }
  </style>
</head>
<body>
<h1 id="another-awesome-dataset-list">Another Awesome Dataset List</h1>
<p><a href="https://github.com/sindresorhus/awesome"><img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" alt="Awesome" /></a></p>
<p><span class="emoji" data-emoji="sparkling_heart">ğŸ’–</span>:</p>
<ul>
<li>AIå¼€å‘è€…ç¥å™¨! è°·æ­Œé‡ç£…æ¨å‡ºæ•°æ®é›†æœç´¢ Dataset Search: <a href="https://mp.weixin.qq.com/s/ErbwXAz-_AJrmUGMHZIcwg">https://mp.weixin.qq.com/s/ErbwXAz-_AJrmUGMHZIcwg</a></li>
<li>Making it easier to discover datasets: <a href="https://www.blog.google/products/search/making-it-easier-discover-datasets/">https://www.blog.google/products/search/making-it-easier-discover-datasets/</a></li>
</ul>
<blockquote>
<p>Please <strong>cite related paper</strong> if you <strong>use their dataset</strong> <span class="emoji" data-emoji="smile">ğŸ˜„</span></p>
</blockquote>
<ul>
<li><a href="#another-awesome-dataset-list">Another Awesome Dataset List</a>
<ul>
<li><a href="#saliency">Saliency</a>
<ul>
<li><a href="#rgb-saliency-detection">RGB-Saliency Detection</a>
<ul>
<li><a href="#msramsra10kmsra-b">MSRA(MSRA10K/MSRA-B)</a></li>
<li><a href="#sed12">SED1/2</a></li>
<li><a href="#asdmsra1000msra1kneed-some-images">ASD(MSRA1000/MSRA1K)[need some images]</a></li>
<li><a href="#dut-omron">DUT-OMRON</a></li>
<li><a href="#duts">DUTS</a></li>
<li><a href="#hku-isneed-some-iamges">HKU-IS[need some iamges]</a></li>
<li><a href="#sod">SOD</a></li>
<li><a href="#icoseg">iCoSeg</a></li>
<li><a href="#infraredneed-help">Infrared[need help]</a></li>
<li><a href="#imgsal">ImgSal</a></li>
<li><a href="#ecssdcssd">ECSSD/CSSD</a></li>
<li><a href="#thur15k">THUR15K</a></li>
<li><a href="#bruce-aneed-help">Bruce-A[need help]</a></li>
<li><a href="#judd-aneed-help">Judd-A[need help]</a></li>
<li><a href="#pascal-s">PASCAL-S</a></li>
<li><a href="#ucsbneed-help">UCSB[need help]</a></li>
<li><a href="#osieneed-help">OSIE[need help]</a></li>
<li><a href="#acsd">ACSD</a></li>
</ul></li>
<li><a href="#other-special-sod-datasets">Other Special SOD Datasets</a>
<ul>
<li><a href="#xpie">XPIE</a></li>
<li><a href="#soc">SOC</a></li>
<li><a href="#sosmosneed-some-images">SOS/MOS[need some images]</a></li>
<li><a href="#ilsoneed-some-images">ILSO[need some images]</a></li>
<li><a href="#hs-sod">HS-SOD</a></li>
</ul></li>
<li><a href="#video-saliency-detection">Video Saliency Detection</a>
<ul>
<li><a href="#rsdpku-rsd">RSD(PKU-RSD)</a></li>
<li><a href="#stcneed-help">STC[need help]</a></li>
</ul></li>
<li><a href="#rgbd-saliency-detection">RGBD-Saliency Detection</a>
<ul>
<li><a href="#sip">SIP</a></li>
<li><a href="#nlprrgbd1000">NLPR/RGBD1000</a></li>
<li><a href="#nju4002000">NJU400/2000</a></li>
<li><a href="#stereossb">STEREO/SSB</a></li>
<li><a href="#lfsdnead-img">LFSD[nead img]</a></li>
<li><a href="#rgbd135des">RGBD135/DES</a></li>
<li><a href="#dut-rgbd">DUT-RGBD</a></li>
<li><a href="#ssd100">SSD100</a></li>
</ul></li>
<li><a href="#rgbt-saliency-detection-need-more-information">RGBT-Saliency Detection [need more information...]</a>
<ul>
<li><a href="#vt1000-dataset">VT1000 Dataset</a></li>
<li><a href="#vt821-dataset">VT821 Dataset</a></li>
</ul></li>
<li><a href="#high-resolution-saliency-detection">High-Resolution Saliency Detection</a>
<ul>
<li><a href="#hrsoddavis-s">HRSOD/DAVIS-S</a></li>
</ul></li>
<li><a href="#other-saliency-dataset">Other Saliency Dataset</a>
<ul>
<li><a href="#kaist-salient-pedestrian-dataset">KAIST Salient Pedestrian Dataset</a></li>
</ul></li>
</ul></li>
<li><a href="#segmentation">Segmentation</a>
<ul>
<li><a href="#generalneed-help">General[need help]</a>
<ul>
<li><a href="#davis">DAVIS</a></li>
<li><a href="#anyu">aNYU</a></li>
</ul></li>
<li><a href="#about-person">About Person</a>
<ul>
<li><a href="#superviselyäººåƒæ•°æ®é›†">Superviselyäººåƒæ•°æ®é›†</a></li>
<li><a href="#clothing-parsing">Clothing Parsing</a></li>
<li><a href="#humanparsing-dataset">HumanParsing-Dataset</a></li>
<li><a href="#look-into-person-lip">Look into Person (LIP)</a></li>
<li><a href="#taobao-commodity-dataset">Taobao Commodity Dataset</a></li>
<li><a href="#object-extraction-dataset">Object Extraction Dataset</a></li>
<li><a href="#clothing-co-parsing-ccp-dataset">Clothing Co-Parsing (CCP) Dataset</a></li>
<li><a href="#baidu-people-segmentation-datasetneed-help">Baidu People segmentation dataset[need help]</a></li>
</ul></li>
</ul></li>
<li><a href="#matting">Matting</a>
<ul>
<li><a href="#alphamattingcom">alphamatting.com</a></li>
<li><a href="#composition-1k-deep-image-matting">Composition-1k: Deep Image Matting</a></li>
<li><a href="#semantic-human-matting">Semantic Human Matting</a></li>
<li><a href="#matting-human-datasets">Matting-Human-Datasets</a></li>
<li><a href="#pfcn">PFCN</a></li>
<li><a href="#deep-automatic-portrait-matting">Deep Automatic Portrait Matting</a></li>
</ul></li>
<li><a href="#other">Other</a>
<ul>
<li><a href="#large-scale-fashion-deepfashion-database">Large-scale Fashion (DeepFashion) Database</a></li>
<li><a href="#ml-image">ML-Image</a></li>
</ul></li>
<li><a href="#need-your-help">need your help...</a></li>
<li><a href="#reference">Reference</a>
<ul>
<li><a href="#salient-object-detection-a-survey">Salient Object Detection: A Survey</a></li>
<li><a href="#review-of-visual-saliency-detection-with-comprehensive-information">Review of Visual Saliency Detection with Comprehensive Information</a></li>
<li><a href="#salient-object-detection-in-the-deep-learning-era-an-in-depth-survey">Salient Object Detection in the Deep Learning Era: An In-Depth Survey</a></li>
</ul></li>
<li><a href="#more">More</a>
<ul>
<li><a href="#similiar-projects">Similiar Projects</a></li>
<li><a href="#research-institutes">Research Institutes</a></li>
<li><a href="#resource-websites">Resource Websites</a></li>
</ul></li>
<li><a href="#about">About</a></li>
</ul></li>
</ul>
<h2 id="saliency">Saliency</h2>
<h3 id="rgb-saliency-detection">RGB-Saliency Detection</h3>
<h4 id="msramsra10kmsra-b">MSRA(MSRA10K/MSRA-B)</h4>
<p><img src="https://mmcheng.net/wp-content/uploads/2014/07/MSRA10K.jpg" alt="img" /></p>
<ul>
<li>è®ºæ–‡: <a href="http://mmlab.ie.cuhk.edu.hk/2007/CVPR07_detect.pdf">T. Liu, J. Sun, N. Zheng, X. Tang, and H.-Y. Shum, "Learningto detect a salient object, " inCVPR, 2007, pp.1â€“8</a></li>
<li>ä¸»é¡µ: å—å¼€å¤§å­¦åª’ä½“è®¡ç®—å®éªŒå®¤: <a href="https://mmcheng.net/zh/msra10k/">https://mmcheng.net/zh/msra10k/</a></li>
<li>ä¸‹è½½:
<ul>
<li>MSRA10K(formally named as THUS10000; <a href="http://mftp.mmcheng.net/Data/MSRA10K_Imgs_GT.zip">195MB</a>: images + binary masks):
<ul>
<li>Pixel accurate salient object labeling for <strong>10000 images</strong> from MSRA dataset.</li>
<li>Please cite our paper [https://mmcheng.net/SalObj/] if you use it.</li>
<li>Saliency maps and salient object region segmentation for other 20+ alternative methods are also available (<a href="http://pan.baidu.com/s/1dEaQqlF#path=%252FShare%252FSalObjRes">ç™¾åº¦ç½‘ç›˜</a>).</li>
</ul></li>
<li>MSRA-B (<a href="http://mftp.mmcheng.net/Data/MSRA-B.zip">111MB</a>: images + binary masks):
<ul>
<li>Pixel accurate salient object labeling for <strong>5000 images</strong> from MSRA-B dataset.</li>
<li>Please cite the corresponding paper [https://mmcheng.net/drfi/] if you use it.</li>
</ul></li>
</ul></li>
</ul>
<blockquote>
<p>æˆ‘ä»¬é€šè¿‡æ£€æµ‹è¾“å…¥å›¾åƒä¸­çš„æ˜¾ç€å¯¹è±¡æ¥ç ”ç©¶è§†è§‰æ³¨æ„åŠ›. æˆ‘ä»¬å°†æ˜¾ç€å¯¹è±¡æ£€æµ‹è¡¨ç¤ºä¸ºå›¾åƒåˆ†å‰²é—®é¢˜, æˆ‘ä»¬å°†æ˜¾ç€å¯¹è±¡ä¸å›¾åƒèƒŒæ™¯åˆ†å¼€. æˆ‘ä»¬æå‡ºäº†ä¸€ç³»åˆ—æ–°é¢–çš„ç‰¹å¾, åŒ…æ‹¬å¤šå°ºåº¦å¯¹æ¯”åº¦, ä¸­å¿ƒç¯ç»•ç›´æ–¹å›¾å’Œé¢œè‰²ç©ºé—´åˆ†å¸ƒ, ä»¥åœ¨æœ¬åœ°, åŒºåŸŸå’Œå…¨å±€æè¿°æ˜¾ç€å¯¹è±¡. å­¦ä¹ æ¡ä»¶éšæœºåœºä»¥æœ‰æ•ˆåœ°ç»„åˆè¿™äº›ç‰¹å¾ä»¥ç”¨äºæ˜¾ç€å¯¹è±¡æ£€æµ‹. æˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ª<strong>åŒ…å«ç”±å¤šä¸ªç”¨æˆ·æ ‡è®°çš„æ•°ä»¥ä¸‡è®¡çš„å®Œå…¨æ ‡è®°å›¾åƒçš„å›¾åƒæ•°æ®åº“</strong>. æ®æˆ‘ä»¬æ‰€çŸ¥, å®ƒæ˜¯ç¬¬ä¸€ä¸ªç”¨äºè§†è§‰æ³¨æ„ç®—æ³•å®šé‡è¯„ä¼°çš„å¤§å‹å›¾åƒæ•°æ®åº“. æˆ‘ä»¬åœ¨æ­¤å›¾åƒæ•°æ®åº“ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•, è¯¥æ•°æ®åº“åœ¨æœ¬æ–‡ä¸­æ˜¯å…¬å¼€çš„.</p>
<p>äººä»¬å¯èƒ½å¯¹å›¾åƒä¸­çš„æ˜¾ç€å¯¹è±¡æœ‰ä¸åŒçš„çœ‹æ³•. ä¸ºäº†è§£å†³"ç»™å®šå›¾åƒä¸­å¯èƒ½æ˜¯ä»€ä¹ˆæ ·çš„æ˜¾ç€å¯¹è±¡"çš„é—®é¢˜, æˆ‘ä»¬é€šè¿‡åœ¨å¤šä¸ªç”¨æˆ·çš„å›¾åƒä¸­æ ‡è®°"åŸºç¡€äº‹å®"æ˜¾ç€å¯¹è±¡æ¥è¿›è¡ŒæŠ•ç¥¨ç­–ç•¥. åœ¨æœ¬æ–‡ä¸­, æˆ‘ä»¬å…³æ³¨å›¾åƒä¸­å•ä¸ªæ˜¾ç€å¯¹è±¡çš„æƒ…å†µ.</p>
<p>æ˜¾è‘—æ€§å¯¹è±¡è¡¨ç¤º. é€šå¸¸, æˆ‘ä»¬<strong>å°†ç»™å®šå¯¹è±¡è¡¨ç¤ºä¸ºç»™å®šimage Iä¸­çš„äºŒå…ƒmask</strong> $A={a_x}$. å¯¹äºæ¯ä¸ªåƒç´ x, $a_xâˆˆ{1, 0}$æ˜¯äºŒè¿›åˆ¶æ ‡ç­¾, ä»¥æŒ‡ç¤ºåƒç´ æ˜¯å¦å±äºæ˜¾ç€å¯¹è±¡.<strong>ä¸ºäº†æ ‡è®°å’Œè¯„ä¼°, æˆ‘ä»¬è¦æ±‚ç”¨æˆ·ç»˜åˆ¶ä¸€ä¸ªçŸ©å½¢æ¥æŒ‡å®šä¸€ä¸ªæ˜¾ç€å¯¹è±¡. æˆ‘ä»¬çš„æ£€æµ‹ç®—æ³•ä¹Ÿè¾“å‡ºä¸€ä¸ªçŸ©å½¢.</strong></p>
<p>å›¾åƒæ¥æº. æˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªéå¸¸å¤§çš„å›¾åƒæ•°æ®åº“, å…¶ä¸­130, 099ä¸ªæ¥è‡ªå„ç§æ¥æºçš„é«˜è´¨é‡å›¾åƒ, ä¸»è¦æ¥è‡ªå›¾åƒè®ºå›å’Œå›¾åƒæœç´¢å¼•æ“. ç„¶åæˆ‘ä»¬æ‰‹åŠ¨é€‰æ‹©60, 000å¤šä¸ªå›¾åƒ, æ¯ä¸ªå›¾åƒåŒ…å«ä¸€ä¸ªæ˜¾ç€å¯¹è±¡æˆ–ä¸€ä¸ªç‹¬ç‰¹çš„å‰æ™¯å¯¹è±¡. æˆ‘ä»¬è¿›ä¸€æ­¥é€‰æ‹©äº†20, 840å¼ å›¾ç‰‡è¿›è¡Œæ ‡è®°. åœ¨é€‰æ‹©è¿‡ç¨‹ä¸­, æˆ‘ä»¬<strong>æ’é™¤äº†åŒ…å«éå¸¸å¤§çš„æ˜¾ç€å¯¹è±¡çš„ä»»ä½•å›¾åƒ</strong>, ä»è€Œå¯ä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°æ£€æµ‹çš„æ€§èƒ½.</p>
<p>æ ‡è®°ä¸€è‡´æ€§. å¯¹äºæ¯ä¸ªè¦æ ‡è®°çš„å›¾åƒ, æˆ‘ä»¬è¯·ç”¨æˆ·ç»˜åˆ¶ä¸€ä¸ªçŸ©å½¢, è¯¥çŸ©å½¢åŒ…å›´å›¾åƒä¸­æœ€å¤§çš„å¯¹è±¡æ ¹æ®ä»–/å¥¹è‡ªå·±çš„ç†è§£. ç”±ä¸åŒç”¨æˆ·æ ‡è®°çš„çŸ©å½¢é€šå¸¸ä¸ç›¸åŒ. ä¸ºäº†å‡å°‘æ ‡ç­¾çš„ä¸ä¸€è‡´æ€§, æˆ‘ä»¬ä»å¤šä¸ªç”¨æˆ·ç»˜åˆ¶çš„çŸ©å½¢ä¸­é€‰æ‹©ä¸€ä¸ª"çœŸå®"æ ‡ç­¾.</p>
</blockquote>
<h4 id="sed12">SED1/2</h4>
<ul>
<li>å•ç›®æ ‡</li>
</ul>
<p><img src="./assets/2018-12-29-18-38-59.png" alt="img" /></p>
<ul>
<li>åŒç›®æ ‡</li>
</ul>
<p><img src="./assets/2018-12-29-18-39-30.png" alt="img" /></p>
<ul>
<li>çœŸå€¼</li>
</ul>
<p>ç»™å‡ºçš„æ˜¯æ¯ä¸ªå›¾åƒç”±ä¸‰ä¸ªä¸åŒçš„äººç±»å¯¹è±¡åˆ†å‰²çš„ç»“æœ.</p>
<p><img src="./assets/2018-12-29-18-40-17.png" alt="img" /></p>
<ul>
<li><a href="https://arxiv.org/abs/1501.02741">A. Borji, M.-M. Cheng, H. Jiang, and J. Li, "Salient objectdetection: A benchmark, "IEEE TIP, vol.24, no.12, pp.5706â€“5722, 2015.</a></li>
<li><a href="http://www.wisdom.weizmann.ac.il/~meirav/Segmentation_Alpert_Galun_Brandt_Basri.pdf">Image Segmentation by Probabilistic Bottom-Up Aggregation and Cue Integration</a></li>
<li>é¡¹ç›®: <a href="http://www.wisdom.weizmann.ac.il/~vision/Seg_Evaluation_DB/index.html">http://www.wisdom.weizmann.ac.il/~vision/Seg_Evaluation_DB/index.html</a></li>
<li>ä¸‹è½½: <a href="http://www.wisdom.weizmann.ac.il/~vision/Seg_Evaluation_DB/dl.html">http://www.wisdom.weizmann.ac.il/~vision/Seg_Evaluation_DB/dl.html</a></li>
</ul>
<blockquote>
<p>è¿™é¡¹å·¥ä½œçš„ç›®çš„æ˜¯ä¸ºå›¾åƒåˆ†å‰²ç ”ç©¶æä¾›ç»éªŒå’Œç§‘å­¦ä¾æ®. è¯„ä¼°åˆ†å‰²ç®—æ³•äº§ç”Ÿçš„ç»“æœå…·æœ‰æŒ‘æˆ˜æ€§, å› ä¸ºå¾ˆéš¾æå‡ºæä¾›åŸºç¡€çœŸå®åˆ†å‰²çš„è§„èŒƒæµ‹è¯•é›†. è¿™éƒ¨åˆ†æ˜¯å› ä¸ºåœ¨æ—¥å¸¸å¤æ‚å›¾åƒä¸­æ‰‹åŠ¨æç»˜ç‰‡æ®µå¯èƒ½æ˜¯è´¹åŠ›çš„. æ­¤å¤–, äººä»¬å¾€å¾€å€¾å‘äºå°†è¯­ä¹‰è€ƒè™‘çº³å…¥å…¶åˆ†æ®µä¸­, è¿™è¶…å‡ºäº†æ•°æ®é©±åŠ¨çš„åˆ†å‰²ç®—æ³•çš„èŒƒå›´. å› æ­¤, è®¸å¤šç°æœ‰ç®—æ³•ä»…æ˜¾ç¤ºå¾ˆå°‘çš„åˆ†å‰²ç»“æœ. ä¸ºäº†è¯„ä¼°ç”±ä¸åŒç®—æ³•äº§ç”Ÿçš„åˆ†å‰², æˆ‘ä»¬ç¼–åˆ¶äº†ä¸€ä¸ªæ•°æ®åº“, ç›®å‰<strong>åŒ…å«200ä¸ªç°åº¦å›¾åƒä»¥åŠçœŸå®æ ‡æ³¨åˆ†å‰²</strong>. è¯¥æ•°æ®åº“ä¸“é—¨è®¾è®¡ç”¨äºé¿å…æ½œåœ¨çš„æ¨¡ç³Š, ä»…é€šè¿‡ä»…é€šè¿‡å¼ºåº¦, çº¹ç†æˆ–å…¶ä»–ä½æ°´å¹³çº¿ç´¢åˆå¹¶æ¸…æ™°æç»˜å‰æ™¯ä¸­ä¸å…¶å‘¨å›´ç¯å¢ƒä¸åŒçš„ä¸€ä¸ªæˆ–ä¸¤ä¸ªç‰©ä½“çš„å›¾åƒ. é€šè¿‡è¦æ±‚äººç±»å¯¹è±¡æ‰‹åŠ¨åœ°å°†ç°åº¦å›¾åƒ(è¿˜æä¾›é¢œè‰²æº)åˆ†æˆä¸¤ä¸ªæˆ–ä¸‰ä¸ªç±»åˆ«æ¥è·å¾—åœ°é¢çœŸå®åˆ†å‰², å…¶ä¸­<strong>æ¯ä¸ªå›¾åƒç”±ä¸‰ä¸ªä¸åŒçš„äººç±»å¯¹è±¡åˆ†å‰²</strong>. é€šè¿‡è¯„ä¼°å…¶ä¸çœŸå®åˆ†å‰²çš„ä¸€è‡´æ€§åŠå…¶ç¢ç‰‡é‡æ¥è¯„ä¼°åˆ†å‰². ä¸æ­¤æ•°æ®åº“è¯„ä¼°ä¸€èµ·, æˆ‘ä»¬æä¾›äº†ç”¨äºè¯„ä¼°ç»™å®šåˆ†å‰²ç®—æ³•çš„ä»£ç . è¿™æ ·, ä¸åŒçš„åˆ†å‰²ç®—æ³•å¯èƒ½å…·æœ‰å¯æ¯”è¾ƒçš„ç»“æœä»¥è·å¾—æ›´å¤šç»†èŠ‚, è¯·å‚é˜…"è¯„ä¼°æµ‹è¯•"éƒ¨åˆ†.</p>
</blockquote>
<h4 id="asdmsra1000msra1kneed-some-images">ASD(MSRA1000/MSRA1K)[need some images]</h4>
<ul>
<li>è®ºæ–‡:<a href="https://www.researchgate.net/publication/224312323_A_two-stage_approach_to_saliency_detection_in_images">A two-stage approach to saliency detection inimages</a></li>
<li>ç›¸å…³:
<ul>
<li>T. Liu, J. Sun, N.-N. Zheng, X. Tang, and H.-Y. Shum, "<a href="http://research.microsoft.com/en-us/um/people/jiansun/salientobject/salient_object.htm">Learning to detect a salient object</a>, " in <em>Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</em>, 2007, pp.1â€“8.</li>
<li>R. Achanta, S. Hemami, F. Estrada, and S. SÃ¼sstrunk, "<a href="http://ivrlwww.epfl.ch/supplementary_material/RK_CVPR09/">Frequency-tuned salient region detection</a>, " in <em>Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</em>, 2009, pp.1597â€“1604.</li>
</ul></li>
<li>ä¸‹è½½: <a href="http://download.csdn.net/detail/wanyq07/9839322">http://download.csdn.net/detail/wanyq07/9839322</a>
<ul>
<li>å…³äºä¸‹è½½çš„è¯´æ˜: å› ä¸ºåŸºäºMSRAçš„å›¾ç‰‡æ•°æ®é›†, åœ¨å­™å‰‘èµ°äº†ä¹‹å, MARAä¸Šå°±æ²¡äº†ä»–çš„é¡µé¢, ç›¸å…³çš„èµ„æºä¹Ÿå°±æ‰¾ä¸åˆ°äº†. CSDNä¸€ç¯‡åšå®¢æœ‰åˆ†äº«. åŸå›¾ä¸‹è½½åœ°å€:<a href="http://download.csdn.net/detail/tuconghuan/8357509">MSRAå›¾åƒæ•°æ®é›†(1000å¹…å«çœŸå®æ ‡æ³¨)</a>. ä¸Šé¢ä¸‹è½½åˆ°çš„æ ‡æ³¨å›¾å°ºå¯¸è¢«ç»Ÿä¸€æ”¹ä¸º512*512, æ‰€ä»¥è¿™é‡Œåœ¨ç»™ä¸ªåœ°å€:<a href="http://download.csdn.net/detail/zzb4702/9559378">ASDå°ºå¯¸ä¸€è‡´</a></li>
</ul></li>
</ul>
<blockquote>
<p>ASD contains 1, 000 images with pixel-wise ground-truths. The images are selected from the MSRA-A dataset, where only the bounding boxes around salient regions are provided. The accurate salient masks in ASD are created based on object contours.</p>
<p>è¿™ä¸ªæ•°æ®é›†åŒ…å«æœ‰1000å¼ å›¾(MSRA1000)è¿™ä¸ªæ•°æ®åº“æ¥è‡ªäº è¯¥æ•°æ®åº“çš„è¯´æ˜ä»¥åŠä¸€äº›ç®—æ³•(IT, MZ, GB, SR, AC, IG ) çš„ç»“æœå¯ä»¥åœ¨<a href="http://ivrlwww.epfl.ch/supplementary_material/RK_CVPR09/index.html">Frequency-tuned Salient Region Detection</a> (FTç®—æ³• =&gt; è¿™é‡Œæ”¹è¿›çš„æ•°æ®é›†å«åšACSD, ç›¸å…³å¯è§<a href="#ACSD">ACSD</a>éƒ¨åˆ†)ä¸‹è½½, æ­¤å¤–å…¶ä¸­è¿˜åŒ…å«äº†è¿™1000å¼ æµ‹è¯•å›¾çš„çœŸå€¼å›¾.</p>
</blockquote>
<h4 id="dut-omron">DUT-OMRON</h4>
<p><img src="assets/2019-03-22-18-45-56.png" alt="img" /></p>
<ul>
<li>è®ºæ–‡: C. Yang, L. Zhang, H. Lu, X. Ruan, and M.-H. Yang, "<a href="http://saliencydetection.net/dut-omron/">Saliency detection via graph-based manifold ranking</a>, " in <em>Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</em>, 2013, pp.3166â€“3173.</li>
<li>é¡¹ç›®: <a href="http://saliencydetection.net/dut-omron/#outline-container-org0e04792">http://saliencydetection.net/dut-omron/#outline-container-org0e04792</a></li>
<li>ä¸‹è½½: <a href="http://saliencydetection.net/dut-omron/download/DUT-OMRON-image.zip">http://saliencydetection.net/dut-omron/download/DUT-OMRON-image.zip</a></li>
</ul>
<blockquote>
<p>æ•°æ®åº“åŒ…æ‹¬ä»è¶…è¿‡140, 000å¼ å›¾åƒä¸­æ‰‹åŠ¨é€‰æ‹©çš„5, 168ä¸ªé«˜è´¨é‡å›¾åƒ. æˆ‘ä»¬å°†å›¾åƒçš„å¤§å°è°ƒæ•´ä¸ºå®½ä¸º400æˆ–é«˜ä¸º400åƒç´ , å…¶ä¸­å¦ä¸€æ¡è¾¹å°äº400. æˆ‘ä»¬æ•°æ®åº“çš„å›¾åƒå…·æœ‰ä¸€ä¸ªæˆ–å¤šä¸ªæ˜¾ç€å¯¹è±¡å’Œç›¸å¯¹å¤æ‚çš„èƒŒæ™¯. æˆ‘ä»¬å…±æœ‰25åå‚ä¸è€…, ç”¨äºæ±‡æ€»çœŸå€¼, æ¯ä¸ªå›¾åƒæœ‰äº”ä¸ªå‚ä¸è€…æ ‡ç­¾. ä»–ä»¬éƒ½æœ‰æ­£å¸¸æˆ–çŸ«æ­£åˆ°æ­£å¸¸çš„è§†åŠ›å¹¶ä¸”æ„è¯†åˆ°æˆ‘ä»¬å®éªŒçš„ç›®æ ‡. æˆ‘ä»¬ä¸ºæå‡ºçš„æ•°æ®åº“æ„å»ºåƒç´ æ–¹é¢çš„çœŸå®æ ‡æ³¨, è¾¹ç•Œæ¡†, å’Œçœ¼ç›å›ºå®šæ ‡æ³¨çœŸå€¼.</p>
<p>æˆ‘ä»¬çš„æ•°æ®é›†æ˜¯å”¯ä¸€ä¸€ä¸ªå…·æœ‰çœ¼ç›å›ºå®š, è¾¹ç•Œæ¡†å’Œåƒç´ æ–¹é¢çš„å¤§è§„æ¨¡çœŸå®æ ‡æ³¨çš„æ•°æ®é›†. ä¸ASDå’ŒMSRAæ•°æ®é›†ä»¥åŠå…¶ä»–ä¸€äº›çœ¼ç›å›ºå®šæ•°æ®é›†(å³MITå’ŒNUSEFæ•°æ®é›†)ç›¸æ¯”, æ•°æ®é›†ä¸­çš„å›¾åƒæ›´åŠ å›°éš¾, å› æ­¤æ›´å…·æŒ‘æˆ˜æ€§, å¹¶ä¸ºç›¸å…³çš„æ˜¾ç€æ€§ç ”ç©¶æä¾›äº†æ›´å¤šçš„æ”¹è¿›ç©ºé—´.</p>
</blockquote>
<h4 id="duts">DUTS</h4>
<ul>
<li>é¡¹ç›®: <a href="http://saliencydetection.net/duts/">http://saliencydetection.net/duts/</a></li>
</ul>
<blockquote>
<p>...we contribute a large scale data set named DUTS, <strong>containing 10, 553 training images and 5, 019 test images</strong>. All training images are collected from the ImageNet DET training/val sets, while test images are collected from the ImageNet DET test set and the SUN data set.</p>
<p>Both the training and test set contain very challenging scenarios for saliency detection. Accurate pixel-level ground truths are manually annotated by 50 subjects.</p>
<p>To our knowledge, DUTS is currently <strong>the largest saliency detection benchmark</strong> with the explicit training/test evaluation protocol.</p>
<p>For fair comparison in the future research, the training set of DUTS serves as a good candidate for learning DNNs, while the test set and other public data sets can be used for evaluation.</p>
</blockquote>
<h4 id="hku-isneed-some-iamges">HKU-IS[need some iamges]</h4>
<ul>
<li>é¡¹ç›®: <a href="https://i.cs.hku.hk/~gbli/deep_saliency.html">https://i.cs.hku.hk/~gbli/deep_saliency.html</a></li>
<li>è®ºæ–‡: <a href="http://i.cs.hku.hk/~yzyu/publication/mdfsaliency-cvpr15.pdf">Visual Saliency Based on Multiscale Deep Features</a></li>
<li>ä¸‹è½½:
<ul>
<li><a href="https://drive.google.com/open?id=0BxNhBO0S5JCRQ1N6V25VeVh6cHc&amp;authuser=0">Google Drive</a></li>
<li><a href="http://pan.baidu.com/s/1c0EpNfM">Baidu Yun</a></li>
</ul></li>
</ul>
<blockquote>
<p>æ•°æ®é›†åŒ…å«4447ä¸ªå…·æœ‰æ˜¾ç€å¯¹è±¡çš„åƒç´ æ³¨é‡Šçš„å›¾åƒ</p>
<p>è§†è§‰æ˜¾ç€æ€§æ˜¯åŒ…æ‹¬è®¡ç®—æœºè§†è§‰åœ¨å†…çš„è®¤çŸ¥å’Œè®¡ç®—ç§‘å­¦ä¸­çš„ä¸€ä¸ªåŸºæœ¬é—®é¢˜. åœ¨æœ¬æ–‡ä¸­, æˆ‘ä»¬å‘ç°å¯ä»¥ä»ä½¿ç”¨æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œ(CNN)æå–çš„å¤šå°ºåº¦ç‰¹å¾ä¸­å­¦ä¹ é«˜è´¨é‡çš„è§†è§‰æ˜¾ç€æ€§æ¨¡å‹. è§†è§‰è¯†åˆ«ä»»åŠ¡çš„æˆåŠŸ. ä¸ºäº†å­¦ä¹ è¿™æ ·çš„æ˜¾ç€æ€§æ¨¡å‹, æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç¥ç»ç½‘ç»œç»“æ„, å®ƒåœ¨CNNé¡¶éƒ¨å…·æœ‰å®Œå…¨è¿æ¥çš„å±‚, è´Ÿè´£ä¸‰ä¸ªä¸åŒå°ºåº¦çš„ç‰¹å¾æå–. ç„¶å, æˆ‘ä»¬æå‡ºä¸€ç§æ”¹è¿›æ–¹æ³•æ¥å¢å¼ºæˆ‘ä»¬çš„æ˜¾ç€æ€§ç»“æœçš„ç©ºé—´ä¸€è‡´æ€§. æœ€å, é’ˆå¯¹ä¸åŒçº§åˆ«çš„å›¾åƒåˆ†å‰²è®¡ç®—çš„èšåˆå¤šä¸ªæ˜¾ç€æ€§å›¾å¯ä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½, ä»è€Œäº§ç”Ÿæ¯”ç”±å•ä¸ªåˆ†å‰²äº§ç”Ÿçš„æ˜¾ç€æ€§å›¾æ›´å¥½çš„æ˜¾ç€æ€§å›¾. ä¸ºäº†ä¿ƒè¿›å¯¹è§†è§‰æ˜¾ç€æ€§æ¨¡å‹çš„è¿›ä¸€æ­¥ç ”ç©¶å’Œè¯„ä¼°, <strong>æˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªæ–°çš„å¤§å‹æ•°æ®åº“, åŒ…æ‹¬4447ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å›¾åƒåŠå…¶åƒç´ æ˜¾ç€æ€§æ³¨é‡Š</strong>.</p>
</blockquote>
<h4 id="sod">SOD</h4>
<p><img src="assets/2019-03-22-18-46-40.png" alt="img" /></p>
<ul>
<li>é¡¹ç›®: <a href="http://elderlab.yorku.ca/SOD/">http://elderlab.yorku.ca/SOD/</a></li>
<li>ä¸‹è½½
<ul>
<li>å®˜æ–¹: <a href="http://elderlab.yorku.ca/SOD/SOD.zip">http://elderlab.yorku.ca/SOD/SOD.zip</a></li>
<li>ç™¾åº¦äº‘: <a href="https://pan.baidu.com/s/1IMElTPwD4yTo2TMSRU-keQ">https://pan.baidu.com/s/1IMElTPwD4yTo2TMSRU-keQ</a></li>
</ul></li>
</ul>
<blockquote>
<p>æ­¤æ•°æ®é›†æ˜¯åŸºäºBerkeley Segmentation Dataset(BSD)çš„æ˜¾ç€å¯¹è±¡è¾¹ç•Œçš„é›†åˆ. è¦æ±‚ä¸ƒä¸ªå¯¹è±¡é€‰æ‹©BSDä¸­ä½¿ç”¨çš„æ¯ä¸ªå›¾åƒä¸­çš„æ˜¾ç€å¯¹è±¡. æ¯ä¸ªä¸»é¢˜éšæœºæ˜¾ç¤ºä¼¯å…‹åˆ©åˆ†å‰²æ•°æ®é›†çš„å­é›†, ä½œä¸ºåœ¨ç›¸åº”å›¾åƒä¸Šé‡å çš„è¾¹ç•Œ. ç„¶å, å¯ä»¥é€šè¿‡å•å‡»é€‰æ‹©å“ªäº›åŒºåŸŸæˆ–åŒºæ®µå¯¹åº”äºæ˜¾ç€å¯¹è±¡.</p>
<p>å¯¹äºBSDä¸­ä½¿ç”¨çš„300ä¸ªå›¾åƒçš„æ¯ä¸ªå›¾åƒ, éƒ½æœ‰ä¸€ä¸ª.matæ–‡ä»¶å¯ä»¥ç”±Matlabæ‰“å¼€. åŠ è½½æ¯ä¸ªmatæ–‡ä»¶ä¼šå°†ä¸€ä¸ªåä¸º"SES"çš„ç»“æ„è¯»å…¥å†…å­˜, è¯¥ç»“æ„æ˜¯ä»SODä¸­æ¯ä¸ªä¸»é¢˜çš„ä¼šè¯ä¸­æ”¶é›†çš„æ•°æ®æ•°ç»„.</p>
<p><span class="emoji" data-emoji="gift_heart">ğŸ’</span> that the original images are available from the Berkely Segmentation Dataset at: <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/">http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/</a></p>
</blockquote>
<h4 id="icoseg">iCoSeg</h4>
<p><img src="./assets/1546085516505.png" alt="1546085516505" /></p>
<ul>
<li>è®ºæ–‡: <a href="http://chenlab.ece.cornell.edu/projects/touch-coseg/iCoseg_dataset.pdf">http://chenlab.ece.cornell.edu/projects/touch-coseg/iCoseg_dataset.pdf</a></li>
<li>é¡¹ç›®: <a href="http://chenlab.ece.cornell.edu/projects/touch-coseg/">http://chenlab.ece.cornell.edu/projects/touch-coseg/</a></li>
<li>ä¸‹è½½: <a href="http://chenlab.ece.cornell.edu/projects/touch-coseg/CMU_Cornell_iCoseg_dataset.zip">http://chenlab.ece.cornell.edu/projects/touch-coseg/CMU_Cornell_iCoseg_dataset.zip</a></li>
</ul>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†38ç»„(643å¹…å›¾åƒ)ä¸­æœ€å¤§çš„å…¬å¼€å¯ç”¨çš„ co-segmentation, ä»¥åŠåƒç´ æ ‡æ³¨çœŸå€¼.</p>
</blockquote>
<h4 id="infraredneed-help">Infrared[need help]</h4>
<ul>
<li>é¡¹ç›®: <a href="https://ivrl.epfl.ch/research-2/research-downloads/supplementary_material-cvpr11-index-html/">https://ivrl.epfl.ch/research-2/research-downloads/supplementary_material-cvpr11-index-html/</a></li>
<li>è®ºæ–‡: <a href="http://infoscience.epfl.ch/record/167478">http://infoscience.epfl.ch/record/167478</a></li>
<li>ä¸‹è½½: <a href="http://ivrgwww.epfl.ch/supplementary_material/cvpr11/nirscene1.zip">http://ivrgwww.epfl.ch/supplementary_material/cvpr11/nirscene1.zip</a></li>
</ul>
<blockquote>
<p>æˆ‘ä»¬ä½¿ç”¨å¯¹ä¼ ç»ŸSLRç›¸æœºçš„ç®€å•ä¿®æ”¹æ¥æ•è·æ•°ç™¾ä¸ªå½©è‰²(RGB)å’Œè¿‘çº¢å¤–(NIR)åœºæ™¯çš„å›¾åƒ. æˆ‘ä»¬è¡¨æ˜, è¿‘çº¢å¤–ä¿¡æ¯çš„æ·»åŠ å¯¼è‡´åœºæ™¯è¯†åˆ«ä»»åŠ¡ä¸­çš„æ€§èƒ½æ˜¾ç€æé«˜, å¹¶ä¸”å½“ä½¿ç”¨é€‚å½“çš„4ç»´é¢œè‰²è¡¨ç¤ºæ—¶, æ”¹è¿›ä»ç„¶æ›´å¤§. ç‰¹åˆ«åœ°, æˆ‘ä»¬æå‡ºäº†MSIFT-ä¸€ç§å¤šå…‰è°±SIFTæè¿°ç¬¦, å½“ä¸åŸºäºå†…æ ¸çš„åˆ†ç±»å™¨ç»“åˆæ—¶, è¶…è¿‡äº†ç°æœ‰æŠ€æœ¯çš„åœºæ™¯è¯†åˆ«æŠ€æœ¯(ä¾‹å¦‚GIST)åŠå…¶å¤šå…‰è°±æ‰©å±•çš„æ€§èƒ½. æˆ‘ä»¬ä½¿ç”¨æ•°ç™¾ä¸ªRGB-NIRåœºæ™¯å›¾åƒçš„æ–°æ•°æ®é›†å¯¹æˆ‘ä»¬çš„ç®—æ³•è¿›è¡Œäº†å¹¿æ³›çš„æµ‹è¯•, å¹¶å¯¹Torralbaçš„åœºæ™¯åˆ†ç±»æ•°æ®é›†è¿›è¡Œäº†åŸºå‡†æµ‹è¯•.</p>
</blockquote>
<h4 id="imgsal">ImgSal</h4>
<p><img src="./assets/1546087781641.png" alt="1546087781641" /></p>
<ul>
<li>é¡¹ç›®: <a href="https://sites.google.com/site/jianlinudt/saliency-database">https://sites.google.com/site/jianlinudt/saliency-database</a></li>
<li>ä½œè€…ä¸»é¡µ: <a href="http://www.escience.cn/people/jianli/DataBase.html">http://www.escience.cn/people/jianli/DataBase.html</a></li>
</ul>
<blockquote>
<p>æ•°æ®åº“çš„ç‰¹ç‚¹</p>
<p>1.235ä¸ªå½©è‰²å›¾åƒçš„é›†åˆ, åˆ†ä¸ºå…­ä¸ªä¸åŒçš„ç±»åˆ«; 2. æä¾›äººç±»å›ºå®šè®°å½•(æ‰«è§†æ•°æ®)å’Œäººç±»æ ‡è®°ç»“æœ; 3. æ˜“äºä½¿ç”¨.</p>
<p>æˆ‘ä»¬å°†åŒæ—¶è€ƒè™‘ä¸åŒå¤§å°çš„æ˜¾ç€åŒºåŸŸçš„æ£€æµ‹. å®é™…ä¸Š, å¯æ¥å—çš„æ˜¾ç€æ€§æ£€æµ‹å™¨åº”è¯¥æ£€æµ‹å¤§çš„å’Œå°çš„æ˜¾ç€åŒºåŸŸ. æ­¤å¤–, æ˜¾ç€æ€§æ£€æµ‹è¿˜åº”è¯¥å®šä½æ‚ä¹±èƒŒæ™¯ä¸­çš„æ˜¾ç€åŒºåŸŸå’Œå…·æœ‰é‡å¤å¹²æ‰°ç‰©çš„åŒºåŸŸ. æˆ‘ä»¬è¿˜æ³¨æ„åˆ°, å¯¹äºä»»ä½•æ˜¾ç€æ€§æ£€æµ‹å™¨, ä¸åŒçš„å›¾åƒå‘ˆç°ä¸åŒçš„éš¾åº¦. ä½†æ˜¯, ç°æœ‰çš„æ˜¾ç€æ€§åŸºå‡†(ä¾‹å¦‚Bruceçš„æ•°æ®é›†, Hou'dataset, Harelçš„æ•°æ®é›†ç­‰)æ˜¯å›¾åƒé›†åˆ, æ²¡æœ‰å°è¯•å¯¹æ‰€éœ€åˆ†æçš„éš¾åº¦è¿›è¡Œåˆ†ç±». å› æ­¤, æˆ‘ä»¬ä¸ºæ˜¾ç€æ€§æ¨¡å‹éªŒè¯åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„æ˜¾ç€æ€§åŸºå‡†. è¯¥æ•°æ®åº“æä¾›REGIONåŸºç¡€äº‹å®(äººç±»æ ‡è®°)å’ŒFIXATIONåŸºç¡€äº‹å®(é€šè¿‡çœ¼åŠ¨ä»ª).</p>
<p>å›¾åƒé›†ä½¿ç”¨Googleä»¥åŠå‚è€ƒæœ€è¿‘çš„æ–‡çŒ®æ”¶é›†äº†åŒ…å«235å¼ å›¾åƒçš„æ•°æ®åº“. æ­¤æ•°æ®åº“ä¸­çš„å›¾åƒä¸º480 x 640åƒç´ , åˆ†ä¸º6ç±»:1)50ä¸ªå…·æœ‰å¤§æ˜¾ç€åŒºåŸŸçš„å›¾åƒ; 2)å…·æœ‰ä¸­é—´æ˜¾ç€åŒºåŸŸçš„80å¹…å›¾åƒ; 3)å…·æœ‰å°æ˜¾ç€åŒºåŸŸçš„60å¹…å›¾åƒ; 4)èƒŒæ™¯æ‚ä¹±çš„15å¹…å›¾åƒ; 5)å¸¦æœ‰é‡å¤å¹²æ‰°ç‰©çš„15å¼ å›¾åƒ; 6)å…·æœ‰å¤§å’Œå°æ˜¾ç€åŒºåŸŸçš„15ä¸ªå›¾åƒ.</p>
</blockquote>
<h4 id="ecssdcssd">ECSSD/CSSD</h4>
<p><img src="assets/2019-03-22-18-47-32.png" alt="img" /></p>
<ul>
<li>ä¸‹è½½: <a href="http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html">http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html</a>
<ul>
<li>ECSSD (1000 images)
<ul>
<li><a href="http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/data/ECSSD/images.zip">ECSSD images (64.6MB)</a></li>
<li><a href="http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/data/ECSSD/ground_truth_mask.zip">ECSSD ground truth masks (1.78MB)</a> (Updated on 9 April, 2015)</li>
</ul></li>
<li>CSSD (200 images)
<ul>
<li><a href="http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/data/CSSD/images.zip">CSSD images (18.7MB)</a></li>
<li><a href="http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/data/CSSD/ground_truth_mask.zip">CSSD groud truth masks (0.75MB)</a></li>
</ul></li>
</ul></li>
</ul>
<p>å…¶ä¸­CSSDåŒ…å«äº†200å¼ å›¾, è€ŒECSSDæ˜¯å‰è€…çš„æ‰©å±•é›†åŒ…å«æœ‰1000å¼ å›¾</p>
<blockquote>
<p>è™½ç„¶MSRA-1000çš„å›¾åƒå†…å®¹ç§ç±»ç¹å¤š, ä½†èƒŒæ™¯ç»“æ„ä¸»è¦æ˜¯ç®€å•è€Œæµç•…. ä¸ºäº†è¡¨ç¤ºè‡ªç„¶å›¾åƒé€šå¸¸è½å…¥çš„æƒ…å†µ, æˆ‘ä»¬å°†[1]ä¸­çš„å¤æ‚åœºæ™¯æ˜¾ç€æ€§æ•°æ®é›†(CSSD)æ‰©å±•åˆ°åŒ…å«1000ä¸ªå›¾åƒçš„æ›´å¤§æ•°æ®é›†(ECSSD)[2], å…¶ä¸­åŒ…å«è®¸å¤šè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰ä½†ç»“æ„å¤æ‚çš„å›¾åƒç”¨äºè¯„ä¼°. è¿™äº›å›¾åƒæ˜¯ä»äº’è”ç½‘ä¸Šè·å–çš„, å¹¶è¦æ±‚5ååŠ©æ‰‹åˆ¶ä½œåœ°é¢çœŸç›¸é¢å…·. ä¸Šé¢æ˜¾ç¤ºäº†å‡ ä¸ªå¸¦æœ‰ç›¸åº”æ©æ¨¡çš„ä¾‹å­.</p>
</blockquote>
<h4 id="thur15k">THUR15K</h4>
<p><img src="./assets/1546088375285.png" alt="1546088375285" /></p>
<ul>
<li>è®ºæ–‡: <a href="https://mmcheng.net/zh/gsal/">https://mmcheng.net/zh/gsal/</a></li>
<li>ä¸‹è½½: <a href="https://mmcheng.net/mftp/Data/THUR15000.zip">https://mmcheng.net/mftp/Data/THUR15000.zip</a>
<ul>
<li>ç™¾åº¦äº‘: <a href="https://pan.baidu.com/s/1u-E-8ujnxBz0mdmXsJglvg">https://pan.baidu.com/s/1u-E-8ujnxBz0mdmXsJglvg</a></li>
</ul></li>
</ul>
<blockquote>
<p>æœ‰æ•ˆè¯†åˆ«å¤§å‹å›¾åƒé›†ä¸­çš„æ˜¾ç€å¯¹è±¡å¯¹äºè®¸å¤šåº”ç”¨æ˜¯å¿…ä¸å¯å°‘çš„, åŒ…æ‹¬å›¾åƒæ£€ç´¢, ç›‘è§†, å›¾åƒæ³¨é‡Šå’Œå¯¹è±¡è¯†åˆ«. æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•, å¿«é€Ÿ, æœ‰æ•ˆçš„ç®—æ³•, é€šè¿‡åˆ†æå›¾åƒé›†åˆæ¥å®šä½å’Œåˆ†å‰²æ˜¾ç€å¯¹è±¡. ä½œä¸ºä¸€ä¸ªå…³é”®çš„æ–°é¢–æ€§, æˆ‘ä»¬é€šè¿‡æå–æœ€å¤§åŒ–å›¾åƒé—´ç›¸ä¼¼æ€§å’Œå›¾åƒå†…æ¸…æ™°åº¦çš„æ˜¾ç€å¯¹è±¡(åœ¨é¢„è¿‡æ»¤å›¾åƒçš„é›†åˆä¸­)æ¥å¼•å…¥ç¾¤ä½“æ˜¾ç€æ€§ä»¥å®ç°ä¼˜è¶Šçš„æ— ç›‘ç£æ˜¾ç€å¯¹è±¡åˆ†å‰². ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•, æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§å‹åŸºå‡†æ•°æ®é›†, <strong>è¯¥æ•°æ®é›†åŒ…å«å¤šä¸ªç±»åˆ«çš„15Kå›¾åƒ, é€‚ç”¨äºæ˜¾ç€å¯¹è±¡åŒºåŸŸçš„6000å¤šä¸ªåƒç´ ç²¾ç¡®çš„åœ°é¢å®å†µæ³¨é‡Š</strong>. åœ¨æˆ‘ä»¬çš„æ‰€æœ‰æµ‹è¯•ä¸­, group saliency å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„å•å›¾åƒæ˜¾ç€æ€§ç®—æ³•, ä»è€Œå®ç°æ›´é«˜çš„ç²¾åº¦å’Œæ›´å¥½çš„å›å¿†. æˆ‘ä»¬çš„ç®—æ³•æˆåŠŸå¤„ç†äº†æ¯”ä»»ä½•ç°æœ‰åŸºå‡†æ•°æ®é›†æ›´å¤§çš„è®¢å•çš„å›¾åƒé›†åˆ, åŒ…æ‹¬æ¥è‡ªå„ç§ç½‘ç»œé—´æºçš„å„ç§å¼‚æ„å›¾åƒ.</p>
<p>æˆ‘ä»¬å¼•å…¥äº†åˆ†ç±»å›¾åƒçš„æ ‡è®°æ•°æ®é›†, ç”¨äºè¯„ä¼°åŸºäºè‰å›¾çš„å›¾åƒæ£€ç´¢. æˆ‘ä»¬ä¸º5ä¸ªå…³é”®å­—ä¸­çš„æ¯ä¸€ä¸ªä¸‹è½½äº†å¤§çº¦3000å¼ å›¾åƒ:"è´è¶", "å’–å•¡æ¯", "ç‹—è·³", "é•¿é¢ˆé¹¿"å’Œ"å¹³é¢", ä¸€èµ·åŒ…æ‹¬å¤§çº¦15000å¼ å›¾åƒ.<strong>å¯¹äºæ¯ä¸ªå›¾åƒ, å¦‚æœå­˜åœ¨å…·æœ‰ä¸æŸ¥è¯¢å…³é”®å­—åŒ¹é…çš„æ­£ç¡®å†…å®¹çš„éæ¨¡ç³Šå¯¹è±¡å¹¶ä¸”å¯¹è±¡çš„å¤§éƒ¨åˆ†å¯è§, åˆ™æˆ‘ä»¬æ ‡è®°è¿™æ ·çš„å¯¹è±¡åŒºåŸŸ. ä¸MSRA10Kç±»ä¼¼, æ˜¾ç€åŒºåŸŸä»¥åƒç´ çº§åˆ«æ ‡è®°. æˆ‘ä»¬åªæ ‡è®°å‡ ä¹å®Œå…¨å¯è§çš„å¯¹è±¡çš„æ˜¾ç€å¯¹è±¡åŒºåŸŸ, å› ä¸ºéƒ¨åˆ†é®æŒ¡çš„å¯¹è±¡å¯¹å½¢çŠ¶åŒ¹é…ä¸å¤ªæœ‰ç”¨. ä¸MSRA10Kä¸åŒ, THUR15Kæ•°æ®é›†ä¸åŒ…å«ä¸ºæ•°æ®é›†ä¸­çš„æ¯ä¸ªå›¾åƒæ ‡è®°çš„æ˜¾ç€åŒºåŸŸ, å³, ä¸€äº›å›¾åƒå¯èƒ½æ²¡æœ‰ä»»ä½•æ˜¾ç€åŒºåŸŸ. è¯¥æ•°æ®é›†ç”¨äºè¯„ä¼°åŸºäºå½¢çŠ¶çš„å›¾åƒæ£€ç´¢æ€§èƒ½.</strong></p>
</blockquote>
<h4 id="bruce-aneed-help">Bruce-A[need help]</h4>
<ul>
<li>è®ºæ–‡: <a href="https://papers.nips.cc/paper/2830-saliency-based-on-information-maximization.pdf">https://papers.nips.cc/paper/2830-saliency-based-on-information-maximization.pdf</a></li>
</ul>
<h4 id="judd-aneed-help">Judd-A[need help]</h4>
<ul>
<li>è®ºæ–‡: <a href="http://people.csail.mit.edu/torralba/publications/wherepeoplelook.pdf">http://people.csail.mit.edu/torralba/publications/wherepeoplelook.pdf</a></li>
</ul>
<h4 id="pascal-s">PASCAL-S</h4>
<p><img src="https://ccvl.jhu.edu/datasets/assets/pascal_salient_object.jpg" alt="img" /></p>
<ul>
<li>é¡¹ç›®:
<ul>
<li><a href="https://ccvl.jhu.edu/datasets/">https://ccvl.jhu.edu/datasets/</a></li>
<li><a href="http://www.cbi.gatech.edu/salobj/">http://www.cbi.gatech.edu/salobj/</a></li>
</ul></li>
<li>ä¸‹è½½:
<ul>
<li>ç™¾åº¦äº‘ç›˜: <a href="https://pan.baidu.com/s/1DZcfwCYdeMW4EGawhXQyig">https://pan.baidu.com/s/1DZcfwCYdeMW4EGawhXQyig</a></li>
<li>é¡µé¢: <a href="http://academictorrents.com/details/6c49defd6f0e417c039637475cde638d1363037e">http://academictorrents.com/details/6c49defd6f0e417c039637475cde638d1363037e</a></li>
</ul></li>
</ul>
<blockquote>
<p>å¯¹æ¥è‡ªPASCAL VOCçš„850å¼ å›¾åƒå­é›†çš„è‡ªç”±ä¿®å¤. æ”¶é›†8ä¸ªä¸»é¢˜, 3sè§‚çœ‹æ—¶é—´, Eyelink IIçœ¼åŠ¨ä»ª. å¤§å¤šæ•°ç®—æ³•çš„æ€§èƒ½è¡¨æ˜PASCAL-Sæ¯”å¤§å¤šæ•°æ˜¾ç€æ€§æ•°æ®é›†åå·®æ›´å°.</p>
<p><span class="emoji" data-emoji="broken_heart">ğŸ’”</span> ç”±äºå…¶æ ‡æ³¨çš„çœŸå€¼æœ‰å¤šä¸ªå€¼, å¸¸è§çš„åšæ³•æ˜¯ä½¿ç”¨ <code>255/2</code> å€¼ä½œä¸ºé˜ˆå€¼è¿›è¡Œå¤„ç†å, å†ä½¿ç”¨è¯¥æ•°æ®é›†</p>
</blockquote>
<h4 id="ucsbneed-help">UCSB[need help]</h4>
<ul>
<li>è®ºæ–‡: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3954044/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3954044/</a></li>
<li>ä¸‹è½½: <a href="https://labs.psych.ucsb.edu/eckstein/miguel/research_pages/saliencydata.html">https://labs.psych.ucsb.edu/eckstein/miguel/research_pages/saliencydata.html</a></li>
</ul>
<h4 id="osieneed-help">OSIE[need help]</h4>
<ul>
<li>è®ºæ–‡: <a href="https://jov.arvojournals.org/article.aspx?articleid=2193943">https://jov.arvojournals.org/article.aspx?articleid=2193943</a></li>
</ul>
<blockquote>
<p>å¤§é‡å…ˆå‰çš„æ¨¡å‹ç”¨äºé¢„æµ‹äººä»¬åœ¨è‡ªç„¶åœºæ™¯ä¸­çš„å¤–è§‚, ä¾§é‡äºåƒç´ çº§å›¾åƒå±æ€§. ä¸ºäº†å¼¥åˆè®¡ç®—æ˜¾ç€æ€§æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ä¸äººç±»è¡Œä¸ºä¹‹é—´çš„è¯­ä¹‰å·®è·, æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ˜¾ç€æ€§ä½“ç³»ç»“æ„, å®ƒå°†ä¿¡æ¯åˆ†ä¸ºä¸‰ä¸ªå±‚æ¬¡: åƒç´ çº§å›¾åƒå±æ€§, å¯¹è±¡çº§å±æ€§å’Œè¯­ä¹‰çº§å±æ€§. é€šå¸¸å¿½ç•¥å¯¹è±¡å’Œè¯­ä¹‰çº§åˆ«ä¿¡æ¯, æˆ–è€…ä»…è®¨è®ºå°‘æ•°æ ·æœ¬å¯¹è±¡ç±»åˆ«, å…¶ä¸­ç¼©æ”¾åˆ°å¤§é‡å¯¹è±¡ç±»åˆ«æ˜¯ä¸å¯è¡Œçš„, ä¹Ÿä¸æ˜¯ç¥ç»åˆç†çš„. ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜, è¿™é¡¹å·¥ä½œæ„å»ºäº†ä¸€ä¸ªåŸºæœ¬å±æ€§çš„åŸåˆ™è¯æ±‡è¡¨æ¥æè¿°å¯¹è±¡å’Œè¯­ä¹‰çº§ä¿¡æ¯, ä»è€Œä¸é™åˆ¶æœ‰é™æ•°é‡çš„å¯¹è±¡ç±»åˆ«. æˆ‘ä»¬<strong>å»ºç«‹äº†ä¸€ä¸ªåŒ…å«500ä¸ªå›¾åƒçš„æ–°æ•°æ®é›†, å…¶ä¸­åŒ…å«15ä¸ªè§‚å¯Ÿè€…çš„çœ¼åŠ¨è¿½è¸ªæ•°æ®å’Œ5, 551ä¸ªå…·æœ‰ç²¾ç»†è½®å»“å’Œ12ä¸ªè¯­ä¹‰å±æ€§çš„åˆ†æ®µå¯¹è±¡çš„æ³¨é‡Šæ•°æ®</strong>(å¯åœ¨è®ºæ–‡ä¸­å…¬å¼€è·å¾—). å®éªŒç»“æœè¯æ˜äº†å¯¹è±¡å’Œè¯­ä¹‰çº§ä¿¡æ¯åœ¨é¢„æµ‹è§†è§‰æ³¨æ„åŠ›æ–¹é¢çš„é‡è¦æ€§.</p>
</blockquote>
<h4 id="acsd">ACSD</h4>
<p><img src="./assets/1546135560011.png" alt="1546135560011" /></p>
<ul>
<li>è®ºæ–‡: <a href="https://infoscience.epfl.ch/record/135217/files/1708.pdf">R. Achanta, S. Hemami, F. Estrada, and S. Ssstrunk, "Frequency-tuned salient region detection, " in CVPR, 2009, pp.1597â€“1604</a></li>
<li>é¡¹ç›®: <a href="https://ivrl.epfl.ch/research-2/research-current/research-saliency/supplementary_material-rk_cvpr09-index-html/">https://ivrl.epfl.ch/research-2/research-current/research-saliency/supplementary_material-rk_cvpr09-index-html/</a></li>
<li>ä¸‹è½½: å®˜ç½‘åªæä¾›äº†<a href="https://ivrl.epfl.ch/wp-content/uploads/2018/08/binarymasks.zip">çœŸå€¼æ ‡æ³¨çš„ä¸‹è½½</a>.</li>
</ul>
<p>åŸºäº[ASDæ•°æ®é›†(MSRA1K)]åˆ¶ä½œ.</p>
<blockquote>
<p>æˆ‘ä»¬ä»[Z. Wang and B. Li. A two-stage approach to saliency detection in images. ICASSP 2008.]ä¸­æå‡ºçš„1000ä¸ªå›¾åƒä¸­è·å¾—äº†ä¸€ä¸ªçœŸå®æ•°æ®åº“.[Z. Wang and B. Li. A two-stage approach to saliency detection in images. ICASSP 2008.]ä¸­çš„åŸºæœ¬äº‹å®æ˜¯åœ¨æ˜¾ç€åŒºåŸŸå‘¨å›´çš„ç”¨æˆ·ç»˜åˆ¶çš„çŸ©å½¢. è¿™æ˜¯ä¸å‡†ç¡®çš„, å¹¶å°†å¤šä¸ªå¯¹è±¡åˆäºŒä¸ºä¸€. æˆ‘ä»¬æ‰‹åŠ¨åˆ†å‰²ç”¨æˆ·ç»˜åˆ¶çš„çŸ©å½¢å†…çš„æ˜¾ç€å¯¹è±¡ä»¥è·å¾—äºŒè¿›åˆ¶æ©ç , å¦‚ä¸‹æ‰€ç¤º. è¿™æ ·çš„æ©è†œæ—¢å‡†ç¡®åˆå…è®¸æˆ‘ä»¬æ¸…æ¥šåœ°å¤„ç†å¤šä¸ªæ˜¾ç€å¯¹è±¡.</p>
</blockquote>
<h3 id="other-special-sod-datasets">Other Special SOD Datasets</h3>
<h4 id="xpie">XPIE</h4>
<p><img src="./assets/1546137404871.png" alt="1546137404871" /></p>
<ul>
<li>é“¾æ¥: <a href="https://www.researchgate.net/publication/320971838_What_is_and_What_is_Not_a_Salient_Object_Learning_Salient_Object_Detector_by_Ensembling_Linear_Exemplar_Regressors">C. Xia, J. Li, X. Chen, A. Zheng, and Y. Zhang, "What is and what is not a salient object? Learning salient object detector by ensembling linear exemplar regressors, " in CVPR , 2017, pp.4142â€“4150.</a></li>
<li>å›¢é˜Ÿ: cvteam: <a href="http://cvteam.net/">http://cvteam.net/</a></li>
<li>é¡¹ç›®:<a href="http://cvteam.net/projects/CVPR17-ELE/ELE.html">http://cvteam.net/projects/CVPR17-ELE/ELE.html</a></li>
<li>ä¸‹è½½: <a href="http://cvteam.net/projects/CVPR17-ELE/XPIE.tar.gz">http://cvteam.net/projects/CVPR17-ELE/XPIE.tar.gz</a></li>
</ul>
<blockquote>
<p>æ‰¾å‡ºä»€ä¹ˆæ˜¯ä»€ä¹ˆå’Œä»€ä¹ˆä¸æ˜¯æ˜¾ç€å¯¹è±¡å¯ä»¥æœ‰åŠ©äºåœ¨æ˜¾ç€å¯¹è±¡æ£€æµ‹(SOD)ä¸­å¼€å‘æ›´å¥½çš„ç‰¹å¾å’Œæ¨¡å‹. åœ¨æœ¬æ–‡ä¸­, æˆ‘ä»¬ç ”ç©¶äº†åœ¨æ„å»ºæ–°çš„SODæ•°æ®é›†æ—¶é€‰æ‹©å’Œä¸¢å¼ƒçš„å›¾åƒ, å‘ç°è®¸å¤šç›¸ä¼¼çš„å€™é€‰è€…, å¤æ‚å½¢çŠ¶å’Œä½å¯¹è±¡æ€§æ˜¯å¾ˆå¤šéæ˜¾ç€å¯¹è±¡çš„ä¸‰ä¸ªä¸»è¦å±æ€§. æ­¤å¤–, å¯¹è±¡å¯èƒ½å…·æœ‰ä½¿å…¶æ˜¾ç€çš„ä¸åŒå±æ€§.</p>
<p>ä¸ºäº†å…¨é¢è§£é‡Šä»€ä¹ˆæ˜¯ä»€ä¹ˆå’Œä»€ä¹ˆä¸æ˜¯æ˜¾ç€å¯¹è±¡, ä¸€ä¸ªå¯è¡Œçš„è§£å†³æ–¹æ¡ˆæ˜¯é€šè¿‡è§‚å¯ŸåŒ…å«åœ¨æ•°æ®é›†ä¸­æˆ–ä»æ•°æ®é›†ä¸­ä¸¢å¼ƒçš„å›¾åƒä¸­çš„å¯¹è±¡çš„ä¸»è¦ç‰¹å¾æ¥ç ”ç©¶æ„å»ºæ–°SODæ•°æ®é›†çš„æ•´ä¸ªè¿‡ç¨‹. ä»è¿™äº›è§‚å¯Ÿä¸­, æˆ‘ä»¬å¯ä»¥æ¨æ–­æ˜¾ç€å’Œéæ˜¾ç€å¯¹è±¡çš„å…³é”®å±æ€§ä»¥åŠåŸºäºå›¾åƒçš„SODæ•°æ®é›†ä¸­å¯èƒ½å­˜åœ¨çš„ä¸»è§‚åå·®. ä¸ºæ­¤, æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§çš„SODæ•°æ®é›†(ç§°ä¸ºXPIE)å¹¶è®°å½•æ„å»ºè¿‡ç¨‹ä¸­çš„æ‰€æœ‰ç»†èŠ‚.</p>
<ol>
<li>æˆ‘ä»¬é¦–å…ˆä»ä¸‰ä¸ªæ¥æºæ”¶é›†ä¸‰ç§å›¾åƒ, åŒ…æ‹¬Panoramio, ImageNetå’Œä¸¤ä¸ªfixationæ•°æ®é›†. è¿™äº›æ“ä½œæ˜¯å…¨è‡ªåŠ¨çš„, ä»¥é¿å…å¼•å…¥å¤ªå¤šçš„ä¸»è§‚åè§.</li>
<li>ä¹‹å, æˆ‘ä»¬è°ƒæ•´æ¯ä¸ªå›¾åƒçš„å¤§å°, ä½¿å…¶æœ€å¤§è¾¹é•¿ä¸º300åƒç´ , å¹¶ä¸¢å¼ƒæ‰€æœ‰æœ€å°è¾¹é•¿å°äº128åƒç´ çš„ç°åº¦æˆ–å½©è‰²å›¾åƒ.</li>
<li>æœ€å, æˆ‘ä»¬åœ¨ä¸‰ä¸ªå›¾åƒå­é›†ä¸­è·å¾—29, 600ä¸ªå½©è‰²å›¾åƒ. åˆ†åˆ«è¡¨ç¤ºä¸ºSet-P, Set-I, Set-E.</li>
</ol>
<p><strong>Set-P åŒ…å«8, 800å…·æœ‰åœ°ç†ä¿¡æ¯çš„æ„Ÿå…´è¶£åœ°ç‚¹çš„å›¾åƒ(ä¾‹å¦‚, GPSå’Œæ ‡ç­¾), å…·æœ‰å¯¹è±¡æ ‡ç­¾çš„Set-IåŒ…å«19, 600å›¾åƒ, ä»¥åŠSet-EåŒ…å«1, 200ä¸ªhuman fixationså›¾åƒ</strong>.</p>
<p>å¯¹äºè¿™äº›å›¾åƒ, æˆ‘ä»¬è¦æ±‚ä¸¤ä½å·¥ç¨‹å¸ˆé€šè¿‡ä¸¤ä¸ªé˜¶æ®µå¯¹å…¶è¿›è¡Œæ³¨é‡Š. åœ¨ç¬¬ä¸€é˜¶æ®µ, å›¾åƒè¢«åˆ†é…ä¸€ä¸ªäºŒè¿›åˆ¶æ ‡è®°:'æ˜¯'ç”¨äºåŒ…å«éæ˜ç¡®å¯¹è±¡, å¦åˆ™ä¸º'å¦'. åœ¨ç¬¬ä¸€é˜¶æ®µä¹‹å, æˆ‘ä»¬å°†21, 002å¼ å›¾ç‰‡æ ‡è®°ä¸º"æ˜¯", å¹¶ä¸”8, 598å›¾åƒæ ‡è®°ä¸º"å¦". åœ¨ç¬¬äºŒé˜¶æ®µ, è¿™ä¸¤ä½å·¥ç¨‹å¸ˆè¿›ä¸€æ­¥è¦æ±‚æ‰‹åŠ¨æ ‡è®°æ ‡è®°ä¸º"æ˜¯"çš„10, 000å¼ å›¾åƒä¸­çš„æ˜¾ç€å¯¹è±¡çš„å‡†ç¡®è¾¹ç•Œ. æ³¨æ„æˆ‘ä»¬æœ‰10åå¿—æ„¿è€…å‚ä¸æ•´ä¸ªè¿‡ç¨‹, ä»¥æ£€æŸ¥æ³¨é‡Šçš„è´¨é‡.<strong>æœ€å, æˆ‘ä»¬è·å¾—äº†10, 000å¼ å›¾åƒçš„äºŒè¿›åˆ¶æ©ç </strong>.</p>
<p>å¯è§è®ºæ–‡å†…å®¹ç¬¬2èŠ‚.</p>
</blockquote>
<h4 id="soc">SOC</h4>
<p><img src="./assets/1546081178458.png" alt="1546081178458" /></p>
<p><img src="./assets/1546081446332.png" alt="1546081446332" /></p>
<ul>
<li>é¡¹ç›®: <a href="http://dpfan.net/SOCBenchmark/">http://dpfan.net/SOCBenchmark/</a></li>
<li>è®ºæ–‡:<a href="http://dpfan.net/wp-content/uploads/2018/04/SOCBenchmark.pdf">Salient Objects in Clutter: Bringing Salient Object Detection to the Foreground</a>
<ul>
<li>ä¸­æ–‡: <a href="http://dpfan.net/wp-content/uploads/SOCBenchmarkCN.pdf">http://dpfan.net/wp-content/uploads/SOCBenchmarkCN.pdf</a></li>
</ul></li>
<li>ä¸‹è½½:
<ul>
<li>Overall 6K SOC Dataset (730.2MB) <a href="https://pan.baidu.com/s/1J8_CF7zE1zApqgAR9eS1Dw">Baidu</a><a href="https://drive.google.com/file/d/1hfo33A7diED2dikTpN9o4KnZTxizGdLr/view?usp=sharing">Google</a></li>
<li>3.6K SOC Training Set (441.32MB) <a href="http://dpfan.net/wp-content/uploads/TrainSet.zip">Here</a><a href="https://pan.baidu.com/s/1Mao0piUuqVXAzmJoNtrtAw">Baidu</a><a href="https://drive.google.com/open?id=16jlzeJJ1tawyBLBN5fRiWbh2y_F0iSyP">Google</a></li>
<li>1.2K SOC Validation Set (146.56MB) <a href="http://dpfan.net/wp-content/uploads/ValSet.zip">Here</a><a href="https://pan.baidu.com/s/1mOmiezCpkr5NCQk8ecvGiQ">Baidu</a><a href="https://drive.google.com/open?id=1vAfP8fCAo2a2KwgsmYLn8r8Rk4Lk7Urr">Google</a></li>
<li>1.2K SOC Test Set (141.86MB) <a href="http://dpfan.net/wp-content/uploads/TestSet.zip">Here</a><a href="https://pan.baidu.com/s/10y-dx9HCPQm9fnp-Brswgw">Baidu</a><a href="https://drive.google.com/open?id=1ZdKrsk-S4J6KQyjx-cPeL0HoKXy7CCxG">Google</a></li>
</ul></li>
</ul>
<blockquote>
<p>åœ¨æœ¬æ–‡ä¸­, æˆ‘ä»¬æä¾›äº†æ˜¾ç€å¯¹è±¡æ£€æµ‹(SOD)æ¨¡å‹çš„ç»¼åˆè¯„ä¼°. æˆ‘ä»¬çš„åˆ†æç¡®å®šäº†ç°æœ‰SODæ•°æ®é›†çš„ä¸¥é‡è®¾è®¡åå·®, å‡è®¾æ¯ä¸ªå›¾åƒåœ¨ä½æ‚æ³¢ä¸­åŒ…å«è‡³å°‘ä¸€ä¸ªæ˜æ˜¾çªå‡ºçš„æ˜¾ç€å¯¹è±¡. è¿™æ˜¯ä¸€ä¸ªä¸åˆ‡å®é™…çš„å‡è®¾. åœ¨ç°æœ‰æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°æ—¶, è®¾è®¡åå·®å¯¼è‡´äº†æœ€å…ˆè¿›çš„SODæ¨¡å‹çš„é¥±å’Œé«˜æ€§èƒ½. ç„¶è€Œ, å½“åº”ç”¨äºç°å®ä¸–ç•Œçš„æ—¥å¸¸åœºæ™¯æ—¶, è¿™äº›æ¨¡å‹ä»ç„¶è¿œè¿œä¸èƒ½ä»¤äººæ»¡æ„. æ ¹æ®æˆ‘ä»¬çš„åˆ†æ, æˆ‘ä»¬é¦–å…ˆç¡®å®šäº†å…¨é¢å’Œå¹³è¡¡çš„æ•°æ®é›†åº”è¯¥å®ç°çš„7ä¸ªå…³é”®æ–¹é¢. ç„¶å, æˆ‘ä»¬æå‡ºä¸€ä¸ªæ–°çš„é«˜è´¨é‡æ•°æ®é›†å¹¶æ›´æ–°ä»¥å‰çš„æ˜¾ç€æ€§åŸºå‡†.</p>
<p>å…·ä½“æ¥è¯´, æˆ‘ä»¬çš„æ•°æ®é›†ç§°ä¸ºSOC, Salient Objects in Clutter, <strong>åŒ…æ‹¬æ¥è‡ªæ—¥å¸¸å¯¹è±¡ç±»åˆ«çš„æ˜¾ç€å’Œéæ˜¾ç€å¯¹è±¡çš„å›¾åƒ</strong>. é™¤äº†å¯¹è±¡ç±»åˆ«æ³¨é‡Šä¹‹å¤–, æ¯ä¸ªçªå‡ºå›¾åƒéƒ½ä¼´éšç€åæ˜ ç°å®ä¸–ç•Œåœºæ™¯ä¸­å¸¸è§æŒ‘æˆ˜çš„å±æ€§(ä¾‹å¦‚, å¤–è§‚å˜åŒ–, æ‚ä¹±), å¹¶ä¸”å¯ä»¥å¸®åŠ© 1)æ›´æ·±å…¥åœ°äº†è§£SODé—®é¢˜, 2)è°ƒæŸ¥ä¸“ä¸šäººå‘˜å’ŒSODæ¨¡å‹çš„ç¼ºç‚¹, 3)ä»ä¸åŒçš„è§’åº¦å®¢è§‚åœ°è¯„ä¼°æ¨¡å‹. æœ€å, æˆ‘ä»¬åœ¨SOCæ•°æ®é›†ä¸ŠæŠ¥å‘ŠåŸºäºå±æ€§çš„æ€§èƒ½è¯„ä¼°. æˆ‘ä»¬ç›¸ä¿¡, æˆ‘ä»¬çš„æ•°æ®é›†å’Œç»“æœå°†ä¸ºæœªæ¥çš„æ˜¾ç€ç‰©ä½“æ£€æµ‹ç ”ç©¶å¼€è¾Ÿæ–°çš„æ–¹å‘.</p>
<p>SOC has 6, 000 images with 80 common categories. Half of the images contain salient objects and the others contain none.<strong>Each salient-object-contained image is annotated with instance-level SOD ground-truth, object category (e.g., dog, book), and challenging factors</strong> (e.g., big/small object).<strong>The non-salient object subset has 783 texture images and 2, 217 real-scene images</strong> (e.g., aurora, sky).</p>
</blockquote>
<h4 id="sosmosneed-some-images">SOS/MOS[need some images]</h4>
<ul>
<li>é¡¹ç›®:<a href="http://cs-people.bu.edu/jmzhang/sos.html">http://cs-people.bu.edu/jmzhang/sos.html</a></li>
<li>è®ºæ–‡:
<ul>
<li>SOS: J. Zhang, S. Ma, M. Sameki, S. Sclaroff, M. Betke, Z. Lin, X. Shen, B. Price, and R. Mech, "Salient object subitizing, " in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2015, pp.4045â€“4054.</li>
<li>MOS: J. Zhang, S. Ma, M. Sameki, S. Sclaroff, M. Betke, Z. Lin, X. Shen, B. Price, and R. Mech, "Salient object subitizing, " in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2015, pp.4045â€“4054.</li>
</ul></li>
</ul>
<blockquote>
<p>SOS 10 is created for SOD subitizing [115], i.e., to predict the number of salient objects without an expensive detection process. It contains 6, 900 images selected from:</p>
<ol>
<li>A large-scale hierarchical image database</li>
<li>Sun database: Large-scale scene recognition from abbey to zoo</li>
<li>Microsoft coco: Common objects in context</li>
<li>The pascal visual object classes (voc) challenge results</li>
</ol>
<p>Each image is labeled as containing 0, 1, 2, 3 or 4+ salient objects. SOS is randomly split into a training (5, 520 images) and a test set (1, 380 images).</p>
<p><strong>MSO is a subset of the test set of SOS and contains 1, 224 images</strong>. It has a more balanced distribution regarding the number of salient objects, and each object is annotated with a bounding box.</p>
</blockquote>
<h4 id="ilsoneed-some-images">ILSO[need some images]</h4>
<ul>
<li>é¡¹ç›®:<a href="http://www.sysu-hcp.net/instance-level-salient-object-segmentation/">http://www.sysu-hcp.net/instance-level-salient-object-segmentation/</a></li>
<li>è®ºæ–‡: G. Li, Y. Xie, L. Lin, and Y. Yu, "Instance-level salient object segmentation, " in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp.247â€“256.</li>
</ul>
<blockquote>
<p>ILSO has 1, 000 images with pixel-wise instancelevel saliency annotations and coarse contour labeling, where the benchmark results are generated using MSRNet [Instance-level salient object segmentation]. Most of the images in ILSO are selected from the following datasets to reduce ambiguity over the salient object regions.</p>
<ol>
<li>Visual saliency based on multiscale deep features</li>
<li>Hierarchical saliency detection</li>
<li>Saliency detection via graph-based manifold ranking</li>
<li>Salient object subitizing</li>
</ol>
</blockquote>
<h4 id="hs-sod">HS-SOD</h4>
<p><img src="https://github.com/gistairc/HS-SOD/raw/master/images/poster-QoMEX2018.png" alt="img" /></p>
<p><img src="./assets/2018-12-28-22-16-20.png" alt="eva" /></p>
<ul>
<li>é¡¹ç›®: <a href="https://github.com/gistairc/HS-SOD">https://github.com/gistairc/HS-SOD</a></li>
<li>ä¸‹è½½: <a href="http://data.airc.aist.go.jp/HS-SOD/HS-SOD.zip">http://data.airc.aist.go.jp/HS-SOD/HS-SOD.zip</a> 5.6G</li>
<li>è®ºæ–‡: <a href="https://arxiv.org/abs/1806.11314">Hyperspectral Image Dataset for Benchmarking on Salient Object Detection</a></li>
</ul>
<blockquote>
<p>ä½¿ç”¨æœ‰ç›‘ç£æˆ–æ— ç›‘ç£çš„æ–¹æ³•å¯¹ç€è‰²å¯¹è±¡è¿›è¡Œäº†æ˜¾ç€çš„ç‰©ä½“æ£€æµ‹. æœ€è¿‘, ä¸€äº›ç ”ç©¶è¡¨æ˜, é€šè¿‡åœ¨æ¥è‡ªå¤©ç„¶æ™¯è§‚çš„é«˜å…‰è°±å›¾åƒçš„å¯è§å…‰è°±ä¸­ä½¿ç”¨å…‰è°±ç‰¹å¾, ä¹Ÿå¯ä»¥å®ç°æœ‰æ•ˆçš„æ˜¾ç€å¯¹è±¡æ£€æµ‹. ç„¶è€Œ, è¿™äº›å…³äºé«˜å…‰è°±æ˜¾ç€ç‰©ä½“æ£€æµ‹çš„æ¨¡å‹ä½¿ç”¨ä»å„ç§åœ¨çº¿å…¬å…±æ•°æ®é›†ä¸­é€‰æ‹©çš„æå°‘æ•°æ•°æ®è¿›è¡Œæµ‹è¯•, è¿™äº›æ•°æ®ä¸æ˜¯ä¸ºäº†ç‰©ä½“æ£€æµ‹ç›®çš„è€Œç‰¹åˆ«åˆ›å»ºçš„. å› æ­¤, åœ¨è¿™é‡Œ, æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡å‘å¸ƒ<strong>å…·æœ‰60ä¸ªé«˜å…‰è°±å›¾åƒçš„é›†åˆçš„é«˜å…‰è°±æ˜¾ç€ç‰©ä½“æ£€æµ‹æ•°æ®é›†</strong>ä»¥åŠ<strong>ç›¸åº”çš„åœ°é¢å®å†µäºŒå€¼å›¾åƒ</strong>å’Œ**ä»£è¡¨æ€§çš„å½©è‰²å›¾åƒ(sRGB)**æ¥æŒ‡å¯¼è¯¥é¢†åŸŸ. æˆ‘ä»¬åœ¨æ•°æ®æ”¶é›†è¿‡ç¨‹ä¸­è€ƒè™‘äº†å‡ ä¸ªæ–¹é¢, ä¾‹å¦‚å¯¹è±¡å¤§å°çš„å˜åŒ–, å¯¹è±¡çš„æ•°é‡, å‰æ™¯-èƒŒæ™¯å¯¹æ¯”åº¦, å›¾åƒä¸Šçš„å¯¹è±¡ä½ç½®ç­‰. ç„¶å, æˆ‘ä»¬ä¸ºæ¯ä¸ªé«˜å…‰è°±æ•°æ®å‡†å¤‡äº†çœŸå€¼äºŒè¿›åˆ¶å›¾åƒ, å…¶ä¸­æ˜¾è‘—æ€§ç›®æ ‡è¢«æ ‡è®°ä¸ºå›¾åƒ. æœ€å, æˆ‘ä»¬ä½¿ç”¨æ›²çº¿ä¸‹é¢ç§¯(AUC)åº¦é‡å¯¹æ–‡çŒ®ä¸­ä¸€äº›ç°æœ‰çš„é«˜å…‰è°±æ˜¾ç€æ€§æ£€æµ‹æ¨¡å‹è¿›è¡Œäº†æ€§èƒ½è¯„ä¼°.</p>
<p>è¿™äº›æ•°æ®æ˜¯åœ¨ä¸œäº¬æ¸¯å£ç å¤´å…¬å¸çš„è®¸å¯ä¸‹, åœ¨æ—¥æœ¬ä¸œäº¬å°åœºçš„ä¸œäº¬æµ·æ»¨åŸå¸‚å…¬å›­æ”¶é›†çš„. æˆ‘ä»¬åœ¨2017å¹´8æœˆè‡³9æœˆæœŸé—´çš„å‡ å¤©å†…æ”¶é›†äº†æ•°æ®, å½“æ—¶å¤©æ°”æ™´æœ—æˆ–éƒ¨åˆ†å¤šäº‘. åœ¨æ¯ä¸ªæ•°æ®æ”¶é›†æ—¥, ä½¿ç”¨ä¸‰è„šæ¶å›ºå®šç›¸æœºä»¥æœ€å°åŒ–å›¾åƒä¸Šçš„è¿åŠ¨å¤±çœŸ. æˆ‘ä»¬å°è¯•æ ¹æ®æ—¥å…‰æ¡ä»¶å°½å¯èƒ½åœ°ä¿æŒç›¸æœºè®¾ç½®çš„æ›å…‰æ—¶é—´å’Œå¢ç›Š, åŒæ—¶ä¿æŒåƒç´ å€¼é¥±å’Œåº¦æˆ–å›¾åƒå¯è§æ€§. ä½œä¸ºæ•°æ®é›†ç”¨æˆ·çš„å‚è€ƒ, æˆ‘ä»¬æä¾›ç›¸æœºè®¾ç½®, ä¾‹å¦‚æ–‡æœ¬æ–‡ä»¶ä¸­æ¯ä¸ªå›¾åƒçš„æ›å…‰æ—¶é—´å’Œå¢ç›Šå€¼ä»¥åŠç›¸åº”çš„æ•°æ®. æˆ‘ä»¬ä¹Ÿæ²¡æœ‰å¯¹æ•è·çš„æ³¢æ®µåº”ç”¨æ ‡å‡†åŒ–. å®ƒå¯ä»¥æé«˜å‰æ™¯å’ŒèƒŒæ™¯åŒºåŸŸä¹‹é—´è‰²å½©å¯¹æ¯”åº¦æ›´é«˜çš„é«˜å…‰è°±å›¾åƒçš„è´¨é‡; ä½†æ˜¯, å®ƒä¹Ÿå¯èƒ½é™ä½æ•°æ®é›†åœ¨æ˜¾ç€å¯¹è±¡æ£€æµ‹ä»»åŠ¡ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•çš„éš¾åº¦.</p>
<p>åœ¨è·å¾—å„ç§é«˜å…‰è°±å›¾åƒå, æˆ‘ä»¬ä»å¤§çº¦50ä¸ªä¸åŒçš„åœºæ™¯ä¸­é€‰æ‹©äº†60ä¸ªå›¾åƒ, æ¡ä»¶æ˜¯:i)æˆ‘ä»¬å»é™¤äº†ç”±äºåœºæ™¯ä¸­çš„è¿åŠ¨å¼•èµ·çš„å¤±çœŸå›¾åƒ(å–å†³äºæ›å…‰æ—¶é—´, ä¸€ä¸ªå›¾åƒå¯èƒ½éœ€è¦å‡ ç§’é’Ÿæ‰èƒ½ç”¨äºç›¸æœº), ii)æˆ‘ä»¬è€ƒè™‘äº†å‡ ä¸ªæ–¹é¢, å¦‚æ˜¾ç€ç‰©ä½“å¤§å°çš„å˜åŒ–, å›¾åƒä¸Šç‰©ä½“çš„ç©ºé—´ä½ç½®, æ˜¾ç€ç‰©ä½“çš„æ•°é‡, å‰æ™¯ * èƒŒæ™¯å¯¹æ¯”åº¦, iii)ä¸€äº›å›¾åƒå…·æœ‰ç›¸åŒçš„åœºæ™¯ä½†ç‰©ä½“ä½ç½®, ç‰©è·æˆ–æ•°é‡å¯¹è±¡å„ä¸ç›¸åŒ.</p>
<p>ä¸ºäº†ä¾¿äºæ˜¾ç€ç‰©ä½“æ£€æµ‹ä»»åŠ¡, æˆ‘ä»¬åœ¨å¯è§å…‰è°±å‘¨å›´è£å‰ªå…‰è°±å¸¦, å¹¶åœ¨ä¼ æ„Ÿå™¨æš—å™ªå£°æ ¡æ­£åä»¥".mat"æ–‡ä»¶æ ¼å¼ä¿å­˜æ¯ä¸ªåœºæ™¯çš„è¶…ç«‹æ–¹ä½“. å¦‚[21]ä¸­æ‰€å®šä¹‰, å¯è§å…‰è°±å…·æœ‰380-780nmçš„è‰¯å¥½å¯æ¥å—èŒƒå›´, ä½†ä¹Ÿå¯ä»¥ä½¿ç”¨[3, 4]ä¸­çš„400-700nmèŒƒå›´. ä¸ºäº†ä¿æŒèŒƒå›´å¹¿æ³›å’Œçµæ´»æ€§, æƒ³è¦ä½¿ç”¨æ•°æ®é›†çš„äºº, æˆ‘ä»¬åœ¨[21]ä¸­ä¸ºæˆ‘ä»¬çš„æ•°æ®é›†é€‰æ‹©äº†380 * 780 nmçš„å®šä¹‰èŒƒå›´, å°½ç®¡åœ¨äººç±»è§†è§‰ç³»ç»Ÿçš„è¿™äº›èŒƒå›´çš„è¾¹ç•Œå¤„è§†è§‰åˆºæ¿€å¯èƒ½è¾ƒå¼±. ç„¶å, æˆ‘ä»¬ä½¿ç”¨é«˜å…‰è°±å›¾åƒæ¸²æŸ“sRGBå½©è‰²å›¾åƒ, é€šè¿‡æ ‡è®°æ˜¾ç€å¯¹è±¡çš„è¾¹ç•Œæ¥åˆ›å»ºåœ°é¢çœŸå®æ˜¾ç€å¯¹è±¡äºŒè¿›åˆ¶å›¾åƒ.</p>
</blockquote>
<p>HS-SOD.zip file contains three folders:</p>
<p>1.hyperspectral: containing 60 hyperspectral images with #spatial rows:768 #spatial columns:1024 #spectral channels:81 (data only within visible spectrum: 380 nm -720 nm) 2.color: 60 color images of hyperspectral dataset rendered in sRGB for visualization 3.ground-truth: 60 ground-truth binary images for salient objects</p>
<h3 id="video-saliency-detection">Video Saliency Detection</h3>
<h4 id="rsdpku-rsd">RSD(PKU-RSD)</h4>
<p><img src="https://pkuml.org/wp-content/uploads/2014/12/samples-of-RSD-1024x271.png" alt="samples of RSD" /></p>
<ul>
<li>è®ºæ–‡: <a href="https://ieeexplore.ieee.org/document/5202529">J. Li, Y. Tian, T. Huang, and W. Gao, "A dataset and evaluation methodology for visual saliency in video, " in IEEE ICME, 2009, pp.442â€“445</a></li>
<li>é¡¹ç›®: <a href="https://pkuml.org/resources/dataset.html">https://pkuml.org/resources/dataset.html</a></li>
<li>ä¸‹è½½: <a href="https://pkuml.org/resources/pku-rsd.html">https://pkuml.org/resources/pku-rsd.html</a></li>
</ul>
<blockquote>
<p>æˆ‘ä»¬æ„å»ºäº†è¿™ä¸ªPKU-RSD(åŒºåŸŸæ˜¾ç€æ€§æ•°æ®é›†)æ•°æ®é›†, å¯ä»¥æ•è·æ—¶ç©ºè§†è§‰æ˜¾ç€æ€§, ç”¨äºè¯„ä¼°ä¸åŒçš„è§†é¢‘æ˜¾ç€æ€§æ¨¡å‹. è¯¥æ•°æ®é›†åŒ…å«431ä¸ªçŸ­è§†é¢‘, å…¶æ¶µç›–å„ç§åœºæ™¯(ç›‘è§†, å¹¿å‘Š, æ–°é—», å¡é€š, ç”µå½±ç­‰)ä»¥åŠç”±23ä¸ªä¸»é¢˜æ‰‹åŠ¨æ ‡è®°çš„é‡‡æ ·å…³é”®å¸§ä¸­çš„æ˜¾ç€å¯¹è±¡çš„ç›¸åº”æ³¨é‡Šç»“æœ.</p>
</blockquote>
<h4 id="stcneed-help">STC[need help]</h4>
<ul>
<li>è®ºæ–‡: <a href="https://pdfs.semanticscholar.org/3347/c330ac5586020ebea60823b1fd4e8d68e936.pdf?_ga=2.181072804.269179473.1546092428-61549168.1544104573">Y. Wu, N. Zheng, Z. Yuan, H. Jiang, and T. Liu, "Detection of salient objects with focused attention based on spatial and temporal coherence, " Chinese Science Bulletin, vol.56, pp.1055â€“1062, 2011.</a></li>
<li>ä¸‹è½½: This dataset is freely available from the author</li>
</ul>
<blockquote>
<p>å¯¹è§†é¢‘å†…å®¹çš„ç†è§£å’Œåˆ†æå¯¹äºä¼—å¤šåº”ç”¨ç¨‹åºæ¥è¯´è‡³å…³é‡è¦, åŒ…æ‹¬è§†é¢‘æ‘˜è¦, æ£€ç´¢, å¯¼èˆªå’Œç¼–è¾‘. æ­¤è¿‡ç¨‹çš„ä¸€ä¸ªé‡è¦éƒ¨åˆ†æ˜¯æ£€æµ‹è§†é¢‘ç‰‡æ®µä¸­çš„æ˜¾ç€(é€šå¸¸æ„å‘³ç€é‡è¦å’Œæœ‰è¶£)å¯¹è±¡. ä¸ç°æœ‰æ–¹æ³•ä¸åŒ, æˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†æ˜¾ç€æ€§æµ‹é‡ä¸ç©ºé—´å’Œæ—¶é—´ç›¸å¹²æ€§ç›¸ç»“åˆçš„æ–¹æ³•. ç©ºé—´å’Œæ—¶é—´ä¸€è‡´æ€§çš„æ•´åˆå—åˆ°äººç±»è§†è§‰ä¸­å…³æ³¨ç„¦ç‚¹çš„å¯å‘. åœ¨æ‰€æå‡ºçš„æ–¹æ³•ä¸­, ä½çº§è§†è§‰åˆ†ç»„çº¿ç´¢çš„ç©ºé—´ç›¸å¹²æ€§(ä¾‹å¦‚å¤–è§‚å’Œè¿åŠ¨)æœ‰åŠ©äºæ¯å¸§å¯¹è±¡èƒŒæ™¯åˆ†ç¦», è€Œå¯¹è±¡å±æ€§çš„æ—¶é—´ä¸€è‡´æ€§(ä¾‹å¦‚å½¢çŠ¶å’Œå¤–è§‚)ç¡®ä¿ä¸€è‡´ç‰©ä½“éšæ—¶é—´å®šä½, å› æ­¤è¯¥æ–¹æ³•å¯¹äºæ„å¤–çš„ç¯å¢ƒå˜åŒ–å’Œç›¸æœºæŒ¯åŠ¨æ˜¯é²æ£’çš„. åœ¨<strong>å¼€å‘äº†åŸºäºç²—åˆ°ç»†å¤šå°ºåº¦åŠ¨æ€è§„åˆ’çš„æœ‰æ•ˆä¼˜åŒ–ç­–ç•¥ä¹‹å, æˆ‘ä»¬ä½¿ç”¨å¯ä¸æœ¬æ–‡ä¸€èµ·å…è´¹è·å¾—çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•</strong>. æˆ‘ä»¬å±•ç¤ºäº†ä¸¤ç§ç±»å‹çš„ä¸€è‡´æ€§çš„æœ‰æ•ˆæ€§å’Œäº’è¡¥æ€§, å¹¶è¯æ˜å®ƒä»¬å¯ä»¥æ˜¾ç€æé«˜è§†é¢‘ä¸­æ˜¾ç€å¯¹è±¡æ£€æµ‹çš„æ€§èƒ½.</p>
</blockquote>
<h3 id="rgbd-saliency-detection">RGBD-Saliency Detection</h3>
<blockquote>
<p>è‡´è°¢:</p>
<ul>
<li>@JXingZhao, åœ¨ä»–çš„å·¥ä½œä¸­æ•´ç†å¹¶å…¬å¼€äº†å¤šä¸ªæ•°æ®é›†: <a href="https://github.com/JXingZhao/ContrastPrior">https://github.com/JXingZhao/ContrastPrior</a></li>
<li>@jiwei0921, åœ¨ä»–çš„å·¥ä½œä¸­æ•´ç†å¹¶å…¬å¼€äº†å¤šä¸ªæ•°æ®é›†: <a href="https://github.com/jiwei0921/RGBD-SOD-datasets">https://github.com/jiwei0921/RGBD-SOD-datasets</a></li>
<li><strong>æ›´å…¨é¢çš„å†…å®¹</strong>å¯è§ <a href="http://dpfan.net/d3netbenchmark/">http://dpfan.net/d3netbenchmark/</a></li>
</ul>
</blockquote>
<h4 id="sip">SIP</h4>
<p><img src="assets/2019-09-15-16-26-26.png" /></p>
<ul>
<li>è®ºæ–‡: Rethinking RGB-D Salient Object Detection: Models, Datasets, and Large-Scale Benchmarks:<a href="https://arxiv.org/pdf/1907.06781.pdf">https://arxiv.org/pdf/1907.06781.pdf</a></li>
<li>é¡¹ç›®: <a href="http://dpfan.net/d3netbenchmark/">http://dpfan.net/d3netbenchmark/</a></li>
<li>ä¸‹è½½: è¯·è§é¡¹ç›®ä¸»é¡µ</li>
</ul>
<blockquote>
<p>we carefully collect a new salient person (SIP) dataset, which consists of 1K high-resolution images that cover diverse real-world scenes from various viewpoints, poses, occlusion, illumination, and background.</p>
</blockquote>
<h4 id="nlprrgbd1000">NLPR/RGBD1000</h4>
<p><img src="./assets/1546138815074.png" alt="1546138815074" /></p>
<ul>
<li>è®ºæ–‡: <a href="https://docs.google.com/uc?authuser=0&amp;id=0B1wzzt1_uP1rb250d0t6dVFXWG8&amp;export=download">Rgbd salient object detection: a benchmark and algorithms</a></li>
<li>é¡¹ç›®: <a href="https://sites.google.com/site/rgbdsaliency/home">https://sites.google.com/site/rgbdsaliency/home</a></li>
<li>ä¸‹è½½: <a href="https://sites.google.com/site/rgbdsaliency/dataset">https://sites.google.com/site/rgbdsaliency/dataset</a></li>
</ul>
<blockquote>
<p>NLPR is also called RGBD1000 dataset which including 1, 000 images. There may exist multiple salient objects in each image. The structured light depth images are obtained by the Microsoft Kinect under different illumination conditions.</p>
<p>è™½ç„¶æ·±åº¦ä¿¡æ¯åœ¨äººç±»è§†è§‰ç³»ç»Ÿä¸­èµ·ç€é‡è¦ä½œç”¨, ä½†åœ¨ç°æœ‰çš„è§†è§‰æ˜¾ç€æ€§è®¡ç®—æ¨¡å‹ä¸­å°šæœªå¾—åˆ°å¾ˆå¥½çš„æ¢ç´¢. åœ¨è¿™é¡¹å·¥ä½œä¸­, <strong>æˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„RGBDå›¾åƒæ•°æ®é›†, ä»¥è§£å†³ç›®å‰RGBDæ˜¾ç€ç›®æ ‡æ£€æµ‹ç ”ç©¶ä¸­æ•°æ®ä¸è¶³çš„é—®é¢˜</strong>. ä¸ºäº†ç¡®ä¿å¤§å¤šæ•°ç°æœ‰çš„RGBæ˜¾ç€æ¨¡å‹åœ¨RGBDåœºæ™¯ä¸­ä»ç„¶è¶³å¤Ÿ, æˆ‘ä»¬ç»§ç»­æä¾›ä¸€ä¸ªç®€å•çš„èåˆæ¡†æ¶, å°†ç°æœ‰çš„RGBäº§ç”Ÿçš„æ˜¾ç€æ€§ä¸æ–°çš„æ·±åº¦è¯±å¯¼æ˜¾ç€æ€§ç›¸ç»“åˆ, å‰è€…æ˜¯ä»ç°æœ‰çš„RGBæ¨¡å‹ä¸­ä¼°ç®—çš„, è€Œå‰è€…æ˜¯åè€…åŸºäºæå‡ºçš„å¤šä¸Šä¸‹æ–‡å¯¹æ¯”æ¨¡å‹. æ­¤å¤–, è¿˜æå‡ºäº†ä¸€ç§ä¸“é—¨çš„å¤šé˜¶æ®µRGBDæ¨¡å‹, å…¶è€ƒè™‘äº†æ¥è‡ªä½çº§ç‰¹å¾å¯¹æ¯”åº¦, ä¸­çº§åŒºåŸŸåˆ†ç»„å’Œé«˜çº§å…ˆéªŒå¢å¼ºçš„æ·±åº¦å’Œå¤–è§‚çº¿ç´¢. å¤§é‡å®éªŒè¡¨æ˜, æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®å®šä½RGBDå›¾åƒä¸­çš„æ˜¾ç€å¯¹è±¡, å¹¶ä¸ºç›®æ ‡å¯¹è±¡åˆ†é…ä¸€è‡´çš„æ˜¾ç€æ€§å€¼.</p>
</blockquote>
<h4 id="nju4002000">NJU400/2000</h4>
<p><img src="./assets/1546139249376.png" alt="1546139249376" /></p>
<ul>
<li>è®ºæ–‡:
<ul>
<li><a href="http://mcg.nju.edu.cn/publication/2014/icip14-jur.pdf">NJU400: Depth saliency based on anisotropic center-surround difference</a></li>
<li><a href="http://mcg.nju.edu.cn/publication/2015/spic15-jur.pdf">NJU2000: Depth-aware salient object detection using anisotropic center-surround difference</a></li>
</ul></li>
<li>å›¢é˜Ÿ: <a href="http://mcg.nju.edu.cn/index.html">MGG</a></li>
<li>é¡¹ç›®: <a href="http://mcg.nju.edu.cn/publication/2014/icip14-jur/index.html">http://mcg.nju.edu.cn/publication/2014/icip14-jur/index.html</a></li>
<li>ä¸‹è½½:
<ul>
<li><a href="http://mcg.nju.edu.cn/resource.html">http://mcg.nju.edu.cn/resource.html</a></li>
<li><a href="http://mcg.nju.edu.cn/dataset/nju400.zip">http://mcg.nju.edu.cn/dataset/nju400.zip</a></li>
<li><a href="http://mcg.nju.edu.cn/dataset/nju2000.zip">http://mcg.nju.edu.cn/dataset/nju2000.zip</a></li>
</ul></li>
</ul>
<blockquote>
<p>NJU2000 contains 2003 stereo image pairs with diverse objects and complex, challenging scenarios, along with ground-truth map. The stereo images are gathered from 3D movies, the Internet, and photographs taken by a Fuji W3 stereo camera.</p>
</blockquote>
<h4 id="stereossb">STEREO/SSB</h4>
<p><img src="assets/2019-05-13-19-48-20.png" /></p>
<ul>
<li>è®ºæ–‡: <a href="http://web.cecs.pdx.edu/~fliu/papers/cvpr2012.pdf">Leveraging stereopsis for saliency analysis</a></li>
<li>é¡¹ç›®: <a href="http://web.cecs.pdx.edu/~fliu/">http://web.cecs.pdx.edu/~fliu/</a></li>
<li>ä¸‹è½½: è¯·åˆ°ä¸»é¡µå¯»æ‰¾, éœ€è¦è”ç³»ä½œè€….</li>
</ul>
<blockquote>
<p>SSB is also called STEREO dataset, which consists of 1000 pairs of binocular images.</p>
</blockquote>
<h4 id="lfsdnead-img">LFSD[nead img]</h4>
<ul>
<li>è®ºæ–‡: Saliency detection on light field</li>
<li>é¡¹ç›®: <a href="https://sites.duke.edu/nianyi/publication/saliency-detection-on-light-field/">https://sites.duke.edu/nianyi/publication/saliency-detection-on-light-field/</a></li>
<li>ä¸‹è½½: è¯·åˆ°ä¸»é¡µå¯»æ‰¾, éœ€è¦è”ç³»ä½œè€….</li>
</ul>
<blockquote>
<p>LFSD is a small dataset which contains 100 images with depth information and human labeled ground truths. The depth information was obtained via the Lytro light field camera.</p>
</blockquote>
<h4 id="rgbd135des">RGBD135/DES</h4>
<p><img src="assets/2019-05-23-10-44-38.png" alt="image" /></p>
<p><img src="assets/2019-05-23-10-44-15.png" alt="depth" /></p>
<p><img src="assets/2019-05-23-10-44-59.png" alt="mask" /></p>
<ul>
<li>è®ºæ–‡: <a href="http://delivery.acm.org/10.1145/2640000/2632866/p23-cheng.pdf?ip=202.118.97.210&amp;id=2632866&amp;acc=ACTIVE%20SERVICE&amp;key=BF85BBA5741FDC6E%2E5FC7500D8F9CB386%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1557798709_a26e3faff3faccad6d62e02d79d1921a">Depth enhanced saliency detection method</a></li>
<li>é¡¹ç›®: <a href="https://github.com/HzFu/DES_code">https://github.com/HzFu/DES_code</a></li>
<li>ä¸‹è½½: é¡¹ç›®ä¸»é¡µæä¾›äº†ä¸‹é¢çš„ä¸‹è½½é“¾æ¥è¯¶:
<ul>
<li><a href="https://onedrive.live.com/redir?resid=F3A8A31ABFAC51B0!256&amp;authkey=!AC4-yOEjn0bgrCQ&amp;ithint=file%2crar">https://onedrive.live.com/redir?resid=F3A8A31ABFAC51B0!256&amp;authkey=!AC4-yOEjn0bgrCQ&amp;ithint=file%2crar</a></li>
<li><a href="https://pan.baidu.com/s/1pLv2B8n">https://pan.baidu.com/s/1pLv2B8n</a></li>
</ul></li>
</ul>
<blockquote>
<p>RGBD135 is also named DES which consists of seven indoor scenes and contains 135 indoor images collected by Microsoft Kinect.</p>
</blockquote>
<h4 id="dut-rgbd">DUT-RGBD</h4>
<ul>
<li>è®ºæ–‡: Depth-induced Multi-scale Recurrent Attention Network for Saliency Detection</li>
<li>é¡¹ç›®: <a href="https://github.com/jiwei0921/DMRA_RGBD-SOD">https://github.com/jiwei0921/DMRA_RGBD-SOD</a></li>
<li>ä¸‹è½½: è¯·è§@jiwei0921çš„RGBD-SOD-datasetsä»“åº“</li>
</ul>
<h4 id="ssd100">SSD100</h4>
<p><img src="assets/2019-09-15-16-17-12.png" /></p>
<ul>
<li>è®ºæ–‡: A Three-pathway Psychobiological Framework of Salient Object Detection Using Stereoscopic Technology: <a href="http://dpfan.net/wp-content/uploads/SSD_dataset_ICCVW17.pdf">http://dpfan.net/wp-content/uploads/SSD_dataset_ICCVW17.pdf</a></li>
<li>ä¸‹è½½: è¯·è§@jiwei0921çš„RGBD-SOD-datasetsä»“åº“</li>
</ul>
<blockquote>
<p>Our SSD100 dataset is built on three stereo movies. The movies contain both the indoors and outdoors scenes. We pick up one stereo image pair at each hundred frames. It totally has tens of thousands of stereo image pairs. We make the image acquisition and image annotation independent to each other, we can avoid dataset design bias, namely a specific type of bias that is caused by experimenters unnatural selection of dataset images. The chosen stereo image pairs are based on one principle: choose the one which the computer detect the salient objects within the complex scenes where even the human cannot tell the salient objects at once. After picking up the stereo image pairs, we divide the image pairs into left images and right images both in 960x1080 size. When we build the ground truth of salient objects, we adhere to the following rules: 1) we mark the salient objects, taking the advice of most people; 2) disconnected regions of the same object are labeled separately; 3) we use solid regions to approximate hollow objects, such as bike wheels. Besides, we will expand this dataset continually in future.</p>
</blockquote>
<h3 id="rgbt-saliency-detection-need-more-information">RGBT-Saliency Detection [need more information...]</h3>
<h4 id="vt1000-dataset">VT1000 Dataset</h4>
<p><img src="assets/2019-05-23-10-49-47.png" alt="image" /></p>
<p><img src="assets/2019-05-23-10-50-23.png" alt="thermal" /></p>
<p><img src="assets/2019-05-23-10-50-02.png" alt="mask" /></p>
<ul>
<li>è®ºæ–‡: RGB-T Image Saliency Detection via Collaborative Graph Learning</li>
<li>é¡¹ç›®:<a href="http://chenglongli.cn/people/lcl/dataset-code.html">http://chenglongli.cn/people/lcl/dataset-code.html</a></li>
<li>ä¸‹è½½: å…·ä½“ä¿¡æ¯å¯è§é¡¹ç›®ä¸»é¡µ
<ul>
<li><a href="https://drive.google.com/file/d/1NCPFNeiy1n6uY74L0FDInN27p6N_VCSd/view?usp=sharing">https://drive.google.com/file/d/1NCPFNeiy1n6uY74L0FDInN27p6N_VCSd/view?usp=sharing</a></li>
<li><a href="https://pan.baidu.com/s/1eGQJhvnKnqV1KJ1GY_63NA">https://pan.baidu.com/s/1eGQJhvnKnqV1KJ1GY_63NA</a></li>
</ul></li>
</ul>
<h4 id="vt821-dataset">VT821 Dataset</h4>
<p><img src="assets/2019-05-23-10-50-42.png" alt="image" /></p>
<p><img src="assets/2019-05-23-10-51-00.png" alt="mask" /></p>
<ul>
<li>è®ºæ–‡: A Unified RGB-T Saliency Detection Benchmark: Dataset, Baselines, Analysis and A Novel Approach</li>
<li>é¡¹ç›®:<a href="http://chenglongli.cn/people/lcl/dataset-code.html">http://chenglongli.cn/people/lcl/dataset-code.html</a></li>
<li>ä¸‹è½½: å…·ä½“ä¿¡æ¯å¯è§é¡¹ç›®ä¸»é¡µ
<ul>
<li><a href="https://drive.google.com/file/d/0B4fH4G1f-jjNR3NtQUkwWjFFREk/view?usp=sharing">https://drive.google.com/file/d/0B4fH4G1f-jjNR3NtQUkwWjFFREk/view?usp=sharing</a></li>
<li><a href="http://pan.baidu.com/s/1bpEaeQV">http://pan.baidu.com/s/1bpEaeQV</a></li>
</ul></li>
</ul>
<h3 id="high-resolution-saliency-detection">High-Resolution Saliency Detection</h3>
<h4 id="hrsoddavis-s">HRSOD/DAVIS-S</h4>
<p><img src="assets/2019-09-15-15-54-12.png" /></p>
<ul>
<li>è®ºæ–‡: Towards High-Resolution Salient Object Detection: <a href="https://arxiv.org/pdf/1908.07274.pdf">https://arxiv.org/pdf/1908.07274.pdf</a></li>
<li>é¡¹ç›®:<a href="https://github.com/yi94code/HRSOD">https://github.com/yi94code/HRSOD</a></li>
<li>ä¸‹è½½: å¯è§é¡¹ç›®ä¸»é¡µ
<ul>
<li>HRSOD: <a href="https://drive.google.com/open?id=1bmDGlkzqHoduNigi_GO4Qy9sA9sIaZcY">https://drive.google.com/open?id=1bmDGlkzqHoduNigi_GO4Qy9sA9sIaZcY</a></li>
<li>DAVIS-S: <a href="https://drive.google.com/open?id=1q1H7yoITLS6i2n-PhgYMIxLdjyhge5AR">https://drive.google.com/open?id=1q1H7yoITLS6i2n-PhgYMIxLdjyhge5AR</a></li>
</ul></li>
</ul>
<blockquote>
<p>...we contribute a High-Resolution Salient Object Detection (HRSOD) dataset, containing 1610 training images and 400 test images. The total 2010 images are collected from the website of Flickr with the license of all creative commons. Pixel-level ground truths are manually annotated by 40 subjects. The shortest edge of each image in our HRSOD is more than 1200 pixels.</p>
</blockquote>
<h3 id="other-saliency-dataset">Other Saliency Dataset</h3>
<h4 id="kaist-salient-pedestrian-dataset">KAIST Salient Pedestrian Dataset</h4>
<p><img src="assets/2019-05-23-10-53-57.png" /></p>
<ul>
<li>è®ºæ–‡: Pedestrian Detection from Thermal Images using Saliency Maps</li>
<li>é¡¹ç›®:<a href="https://github.com/Information-Fusion-Lab-Umass/Salient-Pedestrian-Detection">https://github.com/Information-Fusion-Lab-Umass/Salient-Pedestrian-Detection</a></li>
<li>ä¸‹è½½: å…·ä½“è¯¦è§é¡¹ç›®é¡µé¢</li>
</ul>
<blockquote>
<p>We select 1702 images from the training set of the KAIST Multispectral Pedestrian dataset, by sampling every 15th image from all the images captured during the day and every 10thimage from all the images captured during the night, which contain pedestrians. These images were selected in order to have approximately the same number of images captured on both times of the day (913 day images and 789 night images), containing 4170 instances of pedestrians. We manually annotate these images using the VGG Image Annotator tool to generate the ground truth saliency masks based on the location of the bounding boxes on pedestrians in the original dataset. Additionally, we create a set of 362 images with similar annotations from the test set to validate our deep saliency detection networks, with 193 day images and 169 night images, containing 1029 instances of pedestrians.</p>
</blockquote>
<h2 id="segmentation">Segmentation</h2>
<h3 id="generalneed-help">General[need help]</h3>
<h4 id="davis">DAVIS</h4>
<p><img src="assets/2019-03-13-11-01-47.png" alt="img" /></p>
<ul>
<li>é¡¹ç›®:
<ul>
<li>ç«èµ›ä¸»é¡µ: <a href="https://davischallenge.org/index.html">https://davischallenge.org/index.html</a></li>
</ul></li>
<li>è®ºæ–‡: <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Perazzi_A_Benchmark_Dataset_CVPR_2016_paper.pdf">A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation</a></li>
<li>ä¸‹è½½:
<ul>
<li><a href="https://davischallenge.org/davis2016/code.html">DAVIS 2016</a> In each video sequence a single instance is annotated.</li>
<li><a href="https://davischallenge.org/davis2017/code.html">DAVIS 2017</a> In each video sequence multiple instances are annotated.</li>
</ul></li>
</ul>
<h4 id="anyu">aNYU</h4>
<p><img src="./assets/1546153000959.png" alt="img" /></p>
<ul>
<li>é¡¹ç›®: <a href="https://kylezheng.org/research-projects/densesegattobj/">https://kylezheng.org/research-projects/densesegattobj/</a></li>
<li>è®ºæ–‡: <a href="http://kylezheng.org/densesegattobjdataset/denseseg4objatt_CVPR2014_Kyle.pdf">Dense Semantic Image Segmentation with Objects and Attributes</a></li>
<li>ä¸‹è½½:
<ul>
<li>NYU: <a href="http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html">http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html</a></li>
<li>aNYU: <a href="http://www.robots.ox.ac.uk/~szheng/aNYU/aNYU.tar.gz">http://www.robots.ox.ac.uk/~szheng/aNYU/aNYU.tar.gz</a></li>
</ul></li>
</ul>
<blockquote>
<p>æˆ‘ä»¬çš„ç¬¬ä¸€ç»„å®éªŒæ˜¯å…³äºæ¥è‡ªNYU V2 datasetçš„RGBå›¾åƒ. å¦‚å›¾3æ‰€ç¤º, æˆ‘ä»¬æ·»åŠ äº†8ä¸ªé™„åŠ å±æ€§æ ‡ç­¾, å³æœ¨åˆ¶, å½©ç»˜, æ£‰èŠ±, ç»ç’ƒ, å…‰é¢, å¡‘æ–™, é—ªäº®å’Œçº¹ç†. æˆ‘ä»¬è¦æ±‚3ä¸ªæ³¨é‡Šè€…åœ¨æ¯ä¸ªåˆ†å‰²åœ°é¢çœŸå®åŒºåŸŸä¸Šåˆ†é…ææ–™, è¡¨é¢å±æ€§å±æ€§. Wethenå°†3åå·¥ä½œè€…çš„å¤šæ•°ç¥¨ä½œä¸ºæˆ‘ä»¬çš„8ä¸ªé™„åŠ å±æ€§æ ‡ç­¾. æˆ‘ä»¬å°†æ­¤æ‰©å±•æ•°æ®é›†ç§°ä¸ºattribute NYU(aNYU)æ•°æ®é›†.<strong>è¯¥æ•°æ®é›†ä»28ä¸ªä¸åŒçš„å®¤å†…åœºæ™¯ä¸­æ”¶é›†äº†1449ä¸ªå›¾åƒ.</strong> åœ¨æˆ‘ä»¬çš„å®éªŒä¸­, æˆ‘ä»¬é€‰æ‹©äº†å…·æœ‰è¶³å¤Ÿæ•°é‡å®ä¾‹çš„15ä¸ªå¯¹è±¡ç±»å’Œ8ä¸ªå±æ€§æ¥è®­ç»ƒunary potential. æ­¤å¤–, <strong>æˆ‘ä»¬éšæœºåœ°å°†æ•°æ®é›†åˆ†æˆè®­ç»ƒé›†çš„725ä¸ªå›¾åƒ, éªŒè¯é›†çš„100ä¸ª, ä»¥åŠæµ‹è¯•é›†çš„624ä¸ª.</strong></p>
</blockquote>
<h3 id="about-person">About Person</h3>
<h4 id="superviselyäººåƒæ•°æ®é›†">Superviselyäººåƒæ•°æ®é›†</h4>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201804/5acb1719a6252.png?imageMogr2/format/jpg/quality/90" alt="img" /></p>
<ul>
<li>é¡¹ç›®: <a href="https://supervise.ly/">https://supervise.ly/</a></li>
</ul>
<blockquote>
<ul>
<li>æ•°æ®é›† <strong>ç”±5711å¼ å›¾ç‰‡ç»„æˆ, æœ‰6884ä¸ªé«˜è´¨é‡çš„æ ‡æ³¨çš„äººä½“å®ä¾‹</strong>.</li>
<li>ä¸‹é¢çš„æ‰€æœ‰æ­¥éª¤åœ¨Superviselyå†…éƒ¨å®Œæˆçš„, æ²¡æœ‰ä»»ä½•ç¼–ç .</li>
<li>æ›´é‡è¦çš„æ˜¯, è¿™äº›æ­¥éª¤æ˜¯è¢«æˆ‘å†…éƒ¨çš„æ³¨é‡Šå™¨æ‰§è¡Œçš„, æ²¡æœ‰ä»»ä½•æœºå™¨å­¦ä¹ ä¸“ä¸šçŸ¥è¯†. æ•°æ®ç§‘å­¦å®¶ä»…ä»…åªæ˜¯æ§åˆ¶å’Œç®¡ç†è¿™è¿‡ç¨‹.</li>
<li>æ³¨é‡Šç»„ç”±ä¸¤åæˆå‘˜ç»„æˆå¹¶ä¸”è¿™æ•´ä¸ªè¿‡ç¨‹åªèŠ±äº†4å¤©.</li>
</ul>
</blockquote>
<h4 id="clothing-parsing">Clothing Parsing</h4>
<p><img src="http://vision.is.tohoku.ac.jp/~kyamagu/research/clothing_parsing/clothing.png" alt="img" /></p>
<ul>
<li>é¡¹ç›® :
<ul>
<li><a href="http://vision.is.tohoku.ac.jp/~kyamagu/research/clothing_parsing/">http://vision.is.tohoku.ac.jp/~kyamagu/research/clothing_parsing/</a></li>
<li><a href="http://vision.is.tohoku.ac.jp/~kyamagu/research/paperdoll/">http://vision.is.tohoku.ac.jp/~kyamagu/research/paperdoll/</a></li>
</ul></li>
</ul>
<blockquote>
<p>In this paper we demonstrate an effective method for parsing clothing in fashion photographs, an extremely challenging problem due to the large number of possible garment items, variations in configuration, garment appearance, layering, and occlusion. In addition, we provide a large novel dataset and tools for labeling garment items, to enable future research on clothing estimation. Finally, we present intriguing initial results on using clothing estimates to improve pose identification, and demonstrate a prototype application for pose-independent visual garment retrieval.</p>
</blockquote>
<h4 id="humanparsing-dataset">HumanParsing-Dataset</h4>
<p><img src="assets/2019-03-22-19-14-03.png" alt="img" /></p>
<ul>
<li>é¡¹ç›®:
<ul>
<li><a href="https://github.com/lemondan/HumanParsing-Dataset">https://github.com/lemondan/HumanParsing-Dataset</a></li>
<li><a href="http://www.sysu-hcp.net/deep-human-parsing/">http://www.sysu-hcp.net/deep-human-parsing/</a></li>
<li><a href="https://vuhcs.github.io/">https://vuhcs.github.io/</a></li>
</ul></li>
<li>ç»„ç»‡: <a href="http://sysu-hcp.net/">http://sysu-hcp.net/</a></li>
<li>ä¸‹è½½: <a href="http://pan.baidu.com/s/1qY8bToS">http://pan.baidu.com/s/1qY8bToS</a> (kjgk)</li>
</ul>
<blockquote>
<p>This human parsing dataset includes the detailed pixel-wise annotations for fashion images, which is proposed in our TPAMI paper "Deep Human Parsing with Active Template Regression", and ICCV 2015 paper "Human Parsing with Contextualized Convolutional Neural Network". This dataset contains 7700 images. We use 6000 images for training, 1000 for testing and 700 as the validation set.</p>
</blockquote>
<h4 id="look-into-person-lip">Look into Person (LIP)</h4>
<p><img src="assets/2019-03-12-11-18-29.png" alt="img" /></p>
<ul>
<li>é¡¹ç›®: <a href="http://sysu-hcp.net/lip/overview.php">http://sysu-hcp.net/lip/overview.php</a></li>
<li>ä¸‹è½½: ä¸åŒä»»åŠ¡æœ‰ä¸åŒéƒ¨åˆ†, å…·ä½“å¯è§<a href="http://sysu-hcp.net/lip/overview.php">Dataset</a>é¡µé¢</li>
</ul>
<blockquote>
<p>Look into Person (LIP) is a new large-scale dataset, focus on semantic understanding of person. Following are the detailed descriptions.</p>
<p>The dataset contains 50, 000 images with elaborated pixel-wise annotations with 19 semantic human part labels and 2D human poses with 16 key points.</p>
<p>The annotated 50, 000 images are cropped person instances from COCO dataset with size larger than 50 * 50. The images collected from the real-world scenarios contain human appearing with challenging poses and views, heavily occlusions, various appearances and low-resolutions. We are working on collecting and annotating more images to increase diversity.</p>
</blockquote>
<h4 id="taobao-commodity-dataset">Taobao Commodity Dataset</h4>
<p><img src="assets/2019-03-22-19-09-55.png" alt="img" /></p>
<ul>
<li>é¡¹ç›®: <a href="http://www.sysu-hcp.net/taobao-commodity-dataset/">http://www.sysu-hcp.net/taobao-commodity-dataset/</a></li>
<li>ä¸‹è½½:
<ul>
<li><a href="http://www.sysu-hcp.net/wp-content/uploads/2016/03/Imgs_TCD.zip">http://www.sysu-hcp.net/wp-content/uploads/2016/03/Imgs_TCD.zip</a></li>
<li><a href="http://www.sysu-hcp.net/wp-content/uploads/2016/03/Mask_TCD.zip">http://www.sysu-hcp.net/wp-content/uploads/2016/03/Mask_TCD.zip</a></li>
</ul></li>
</ul>
<blockquote>
<p>TCD contains 800 commodity images (dresses, jeans, T-shirts, shoes and hats) from the shops on the Taobao website. The ground truth masks of the TCD dataset are obtained by inviting common sellers of Taobao website to annotate their commodities, i.e., masking salient objects that they want to show from their exhibition. These images include all kind s of commodity with and without human models, thus having complex backgrounds and scenes with highly complex foregrounds. Pixel-accurate ground truth masks are given. These images including all kinds of commodities with and without human models have complex backgrounds and scenes with large foregrounds for evaluation. Figure 1 illustrates some of them.</p>
</blockquote>
<h4 id="object-extraction-dataset">Object Extraction Dataset</h4>
<p><img src="https://objectextraction.github.io/imgs/images_masks.png" alt="img" /></p>
<ul>
<li>é¡¹ç›®: <a href="https://objectextraction.github.io/">https://objectextraction.github.io/</a></li>
</ul>
<blockquote>
<p>This Object Extraction newly collected by us contains 10183 images with groundtruth segmentation masks. We selected the images from the PASCAL, iCoseg, Internet dataset as well as other data (most of them are about people and clothes) from the web. We randomly split the dataset with 8230 images for training and 1953 images for testing.</p>
</blockquote>
<h4 id="clothing-co-parsing-ccp-dataset">Clothing Co-Parsing (CCP) Dataset</h4>
<p><img src="assets/2019-03-12-11-12-28.png" alt="img" /></p>
<ul>
<li>é¡¹ç›®: <a href="https://github.com/bearpaw/clothing-co-parsing">https://github.com/bearpaw/clothing-co-parsing</a></li>
</ul>
<blockquote>
<p>Clothing Co-Parsing (CCP) dataset is a new clothing database including elaborately annotated clothing items. 2, 098 high-resolution street fashion photos with totally 59 tags Wide range of styles, accessaries, garments, and pose All images are with image-level annotations 1000+ images are with pixel-level annotations</p>
</blockquote>
<h4 id="baidu-people-segmentation-datasetneed-help">Baidu People segmentation dataset[need help]</h4>
<ul>
<li>ä¸‹è½½: <a href="http://www.cbsr.ia.ac.cn/users/ynyu/dataset/">http://www.cbsr.ia.ac.cn/users/ynyu/dataset/</a></li>
</ul>
<blockquote>
<p>è¿™ä¸ªæ•°æ®é›†ä¸»è¦æ˜¯ç”¨äºäººä½“æ•´ä½“åˆ†å‰². å®ƒç”±5387å¼ è®­ç»ƒå›¾ç‰‡ç»„æˆ, ä½†æ˜¯æµ‹è¯•å›¾ç‰‡æ²¡æœ‰å…¬å¸ƒ. å› æ­¤è®­ç»ƒæ—¶å¯ä»¥ä»5387ä¸­éšæœºæŒ‘é€‰500å¼ ä½œä¸ºéªŒè¯é›†, ç„¶å4887å¼ ä½œä¸ºè®­ç»ƒé›†. å‚è€ƒè®ºæ–‡ã€ŠEarly Hierarchical Contexts Learned by CNN for image segmentationã€‹.</p>
<p>åŸæ–‡:<a href="https://blog.csdn.net/mou_it/article/details/82225505">https://blog.csdn.net/mou_it/article/details/82225505</a></p>
</blockquote>
<h2 id="matting">Matting</h2>
<h3 id="alphamattingcom">alphamatting.com</h3>
<p><img src="./assets/1546154705536.png" alt="1546154705536" /></p>
<ul>
<li>é¡¹ç›®: <a href="http://alphamatting.com/datasets.php">http://alphamatting.com/datasets.php</a></li>
<li>ä¸‹è½½:
<ul>
<li><a href="http://alphamatting.com/datasets/zip/input_training_lowres.zip">input_training_lowres.zip</a></li>
<li><a href="http://alphamatting.com/datasets/zip/input_training_highres.zip">input_training_highres.zip</a></li>
<li><a href="http://alphamatting.com/datasets/zip/trimap_training_lowres.zip">trimap_training_lowres.zip</a></li>
<li><a href="http://alphamatting.com/datasets/zip/trimap_training_highres.zip">trimap_training_highres.zip</a></li>
<li><a href="http://alphamatting.com/datasets/zip/gt_training_lowres.zip">gt_training_lowres.zip</a></li>
<li><a href="http://alphamatting.com/datasets/zip/gt_training_highres.zip">gt_training_highres.zip</a></li>
</ul></li>
</ul>
<blockquote>
<p>è¿™æ˜¯å›¾åƒmattingæ–¹æ³•çš„ç°æœ‰åŸºå‡†. å®ƒ<strong>åŒ…æ‹¬8ä¸ªæµ‹è¯•å›¾åƒ, æ¯ä¸ªå›¾åƒæœ‰3ä¸ªä¸åŒçš„ä¸‰ç»´å›¾å½¢</strong>, å³"small", "large"å’Œ"user"</p>
</blockquote>
<h3 id="composition-1k-deep-image-matting">Composition-1k: Deep Image Matting</h3>
<p><img src="./assets/1546154519720.png" alt="1546154519720" /></p>
<ul>
<li>é¡¹ç›®: <a href="https://sites.google.com/view/deepimagematting">https://sites.google.com/view/deepimagematting</a></li>
<li>è®ºæ–‡: <a href="https://arxiv.org/abs/1703.03872">https://arxiv.org/abs/1703.03872</a></li>
<li>ä¸‹è½½: Please contact Brian Price (<a href="mailto:bprice@adobe.com">bprice@adobe.com</a>) for the dataset.</li>
</ul>
<blockquote>
<p>æŠ å›¾æ˜¯ä¸€ä¸ªåŸºæœ¬çš„è®¡ç®—æœºè§†è§‰é—®é¢˜, æœ‰è®¸å¤šåº”ç”¨. å½“å›¾åƒå…·æœ‰ç›¸ä¼¼çš„å‰æ™¯è‰²å’ŒèƒŒæ™¯è‰²æˆ–å¤æ‚çš„çº¹ç†æ—¶, å…ˆå‰çš„ç®—æ³•å…·æœ‰å·®çš„æ€§èƒ½. ä¸»è¦åŸå› æ˜¯å…ˆå‰çš„æ–¹æ³• 1)ä»…ä½¿ç”¨ä½çº§åŠŸèƒ½å’Œ2)ç¼ºä¹é«˜çº§ä¸Šä¸‹æ–‡. åœ¨æœ¬æ–‡ä¸­, æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ·±åº¦å­¦ä¹ çš„ç®—æ³•, å¯ä»¥è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜. æˆ‘ä»¬çš„æ·±å±‚æ¨¡å‹æœ‰ä¸¤ä¸ªéƒ¨åˆ†. ç¬¬ä¸€éƒ¨åˆ†æ˜¯æ·±åº¦å·ç§¯ç¼–ç å™¨ * è§£ç å™¨ç½‘ç»œ, å®ƒå°†å›¾åƒå’Œç›¸åº”çš„trimapä½œä¸ºè¾“å…¥å¹¶é¢„æµ‹å›¾åƒçš„alphaé®ç½©. ç¬¬äºŒéƒ¨åˆ†æ˜¯ä¸€ä¸ªå°çš„å·ç§¯ç½‘ç»œ, å®ƒæ”¹è¿›äº†ç¬¬ä¸€ä¸ªç½‘ç»œçš„alphaé®ç½©é¢„æµ‹, ä»¥è·å¾—æ›´å‡†ç¡®çš„alphaå€¼å’Œæ›´æ¸…æ™°çš„è¾¹ç¼˜. æ­¤å¤–, <strong>æˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªå¤§å‹å›¾åƒæŠ å›¾æ•°æ®é›†, åŒ…æ‹¬49300ä¸ªè®­ç»ƒå›¾åƒå’Œ1000ä¸ªæµ‹è¯•å›¾åƒ</strong>. æˆ‘ä»¬åœ¨æŠ å›¾åŸºå‡†, æˆ‘ä»¬çš„æµ‹è¯•é›†å’Œå„ç§çœŸå®å›¾åƒä¸Šè¯„ä¼°æˆ‘ä»¬çš„ç®—æ³•. å®éªŒç»“æœæ¸…æ¥šåœ°è¯æ˜äº†æˆ‘ä»¬çš„ç®—æ³•ä¼˜äºä»¥å‰çš„æ–¹æ³•.</p>
<p>æˆ‘ä»¬ä½¿ç”¨åˆæˆåˆ›å»ºä¸€ä¸ªå¤§è§„æ¨¡çš„mattingæ•°æ®é›†. ä»”ç»†æå–å…·æœ‰ç®€å•èƒŒæ™¯ä¸Šçš„å¯¹è±¡çš„å›¾åƒå¹¶å°†å…¶åˆæˆåˆ°æ–°çš„èƒŒæ™¯å›¾åƒä¸Šä»¥åˆ›å»ºå…·æœ‰49300(45500)ä¸ªè®­ç»ƒå›¾åƒå’Œ1000ä¸ªæµ‹è¯•å›¾åƒçš„æ•°æ®é›†.</p>
<p>......</p>
<p>æˆ‘ä»¬å°†è¯„ä¼°3ä¸ªæ•°æ®é›†ä¸Šçš„æ–¹æ³•.1)æˆ‘ä»¬è¯„ä¼°alphamatting.comæ•°æ®é›†, è¿™æ˜¯å›¾åƒmattingæ–¹æ³•çš„ç°æœ‰åŸºå‡†. å®ƒ<strong>åŒ…æ‹¬8ä¸ªæµ‹è¯•å›¾åƒ, æ¯ä¸ªå›¾åƒæœ‰3ä¸ªä¸åŒçš„ä¸‰ç»´å›¾å½¢</strong>, å³"å°", "å¤§"å’Œ"ç”¨æˆ·".2)ç”±äºalphamatting.comæ•°æ®é›†ä¸­å¯¹è±¡çš„å¤§å°å’ŒèŒƒå›´æœ‰é™, **æˆ‘ä»¬æå‡ºäº†Composition-1kæµ‹è¯•é›†. æˆ‘ä»¬åŸºäºä½œå“çš„æ•°æ®é›†åŒ…æ‹¬1000ä¸ªå›¾åƒå’Œ50ä¸ªç‹¬ç‰¹çš„å‰æ™¯. æ­¤æ•°æ®é›†å…·æœ‰æ›´å¹¿æ³›çš„å¯¹è±¡ç±»å‹å’ŒèƒŒæ™¯åœºæ™¯.**3)ä¸ºäº†æµ‹é‡æˆ‘ä»¬åœ¨è‡ªç„¶å›¾åƒä¸Šçš„è¡¨ç°, æˆ‘ä»¬è¿˜æ”¶é›†äº†åŒ…æ‹¬31ä¸ªè‡ªç„¶å›¾åƒçš„ç¬¬ä¸‰ä¸ªæ•°æ®é›†.</p>
</blockquote>
<h3 id="semantic-human-matting">Semantic Human Matting</h3>
<p><img src="./assets/1546156688347.png" alt="1546156688347" /></p>
<p><img src="./assets/2018-12-27-11-47-26.png" alt="dataset" /></p>
<ul>
<li>è®ºæ–‡: <a href="https://arxiv.org/abs/1809.01354">https://arxiv.org/abs/1809.01354</a></li>
</ul>
<blockquote>
<ul>
<li>alpha matting çš„è³‡æ–™åº«æ¨£æœ¬éå°‘, å°æ–¼æ·±åº¦å­¸ç¿’ä¾†èªªé¦–è¦æ¢ä»¶å°±æ˜¯è³‡æ–™æ¨£æœ¬è¦å¤š</li>
<li>Shen et al. æ­¤è³‡æ–™åº«æ˜¯é€é CF ä»¥åŠ KNN çš„æ–¹å¼æ‰€è£½é€ çš„, å› æ­¤æœ‰å¯èƒ½è©²è³‡æ–™åº«æœ‰bias, ä¸æ¡ç”¨.(é€™éƒ¨åˆ†å¯æœå°‹å¹¾å€‹é—œéµå­—: deep learning dataset bias).</li>
<li>DIM çš„è³‡æ–™åº«é›–ç„¶æœ‰ 493 å€‹ç‰©ä»¶, ä½†æ˜¯ç‰©ä»¶ä¸­åŒ…å«äººç‰©çš„åªæœ‰ 202 å€‹.</li>
<li>Our dataset å¾é›»å­å•†å‹™ç¶²ç«™ä¸­æœé›†åœ–ç‰‡, å°‡35, 513å€‹äººç‰©é€éäººå·¥æ¨™æ³¨ä»–çš„Annotation, æ­¤è³‡æ–™é›†æœ‰éµå¾ªDIMçš„æ–¹æ³•æ”¶é›†.</li>
</ul>
<p><a href="https://medium.com/@xiaosean5408/%E6%B7%98%E5%AF%B6%E7%B6%B2%E7%9A%84%E4%BA%BA%E7%89%A9%E6%8F%90%E5%8F%96%E8%AB%96%E6%96%87%E7%B0%A1%E4%BB%8B-semantic-human-matting-52591c3f8e0c">https://medium.com/@xiaosean5408/%E6%B7%98%E5%AF%B6%E7%B6%B2%E7%9A%84%E4%BA%BA%E7%89%A9%E6%8F%90%E5%8F%96%E8%AB%96%E6%96%87%E7%B0%A1%E4%BB%8B-semantic-human-matting-52591c3f8e0c</a></p>
</blockquote>
<h3 id="matting-human-datasets">Matting-Human-Datasets</h3>
<p><img src="https://github.com/aisegmentcn/matting_human_datasets/raw/master/1.png" /></p>
<p><img src="https://github.com/aisegmentcn/matting_human_datasets/raw/master/2.png" /></p>
<ul>
<li>é¡¹ç›®:<a href="https://github.com/aisegmentcn/matting_human_datasets">https://github.com/aisegmentcn/matting_human_datasets</a></li>
<li>ä¸‹è½½:
<ul>
<li>ç™¾åº¦äº‘ç›˜:<a href="https://pan.baidu.com/s/1R9PJJRT-KjSxh-2-3wCGxQ">https://pan.baidu.com/s/1R9PJJRT-KjSxh-2-3wCGxQ</a> æå–ç :dzsn</li>
<li>mega:<a href="https://mega.nz/#F!Gh8CFAyb!e2ppUh-copP76GbE8IWAEQ">https://mega.nz/#F!Gh8CFAyb!e2ppUh-copP76GbE8IWAEQ</a></li>
<li>kaggle:<a href="https://www.kaggle.com/laurentmih/aisegmentcom-matting-human-datasets/">https://www.kaggle.com/laurentmih/aisegmentcom-matting-human-datasets/</a></li>
</ul></li>
</ul>
<blockquote>
<p>æœ¬æ•°æ®é›†ä¸ºç›®å‰å·²çŸ¥æœ€å¤§çš„äººåƒmattingæ•°æ®é›†, åŒ…å«34427å¼ å›¾åƒå’Œå¯¹åº”çš„mattingç»“æœå›¾. æ•°æ®é›†ç”±åŒ—äº¬ç©æ˜Ÿæ±‡èšç§‘æŠ€æœ‰é™å…¬å¸é«˜è´¨é‡æ ‡æ³¨, ä½¿ç”¨è¯¥æ•°æ®é›†æ‰€è®­ç»ƒçš„äººåƒè½¯åˆ†å‰²æ¨¡å‹å·²å•†ç”¨.</p>
<p>æ•°æ®é›†ä¸­çš„åŸå§‹å›¾ç‰‡æ¥æºäºFlickr, ç™¾åº¦, æ·˜å®. ç»è¿‡äººè„¸æ£€æµ‹å’ŒåŒºåŸŸè£å‰ªåç”Ÿæˆäº†600*800çš„åŠèº«äººåƒ.</p>
<ul>
<li>clip_imgç›®å½•ä¸ºåŠèº«äººåƒå›¾åƒ, æ ¼å¼ä¸ºjpg;</li>
<li>mattingç›®å½•ä¸ºå¯¹åº”çš„mattingæ–‡ä»¶(æ–¹ä¾¿ç¡®è®¤mattingè´¨é‡), æ ¼å¼ä¸ºpng, æ‚¨è®­ç»ƒå‰åº”è¯¥å…ˆä»pngå›¾åƒæå–alphaå›¾. ä¾‹å¦‚ä½¿ç”¨opencvå¯ä»¥è¿™æ ·è·å¾—alphaå›¾:</li>
</ul>
</blockquote>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a>in_image <span class="op">=</span> cv2.imread(<span class="st">&#39;pngå›¾åƒæ–‡ä»¶è·¯å¾„&#39;</span>, cv2.IMREAD_UNCHANGED)</span>
<span id="cb1-2"><a href="#cb1-2"></a>alpha <span class="op">=</span> in_image[:,:,<span class="dv">3</span>]</span></code></pre></div>
<h3 id="pfcn">PFCN</h3>
<p><img src="assets/1546173669466.png" alt="1546173669466" /></p>
<ul>
<li>é¡¹ç›®: <a href="http://xiaoyongshen.me/webpage_portrait/index.html">http://xiaoyongshen.me/webpage_portrait/index.html</a></li>
<li>è®ºæ–‡: <a href="http://xiaoyongshen.me/webpage_portrait/papers/portrait_eg16.pdf">Automatic Portrait Segmentation for Image Stylization</a></li>
<li>ä¸‹è½½: Please download from <a href="https://1drv.ms/u/s!ApwdOxIIFBH19TzDv7nRfH5ZsMNL">OneDrive</a> or <a href="http://pan.baidu.com/s/1bQ4yHC">Baiduyun</a>.</li>
</ul>
<blockquote>
<p>è‚–åƒç”»æ˜¯æ‘„å½±å’Œç»˜ç”»çš„ä¸»è¦è‰ºæœ¯å½¢å¼. åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹, è‰ºæœ¯å®¶è¯•å›¾ä½¿ä¸»ä½“ä»å‘¨å›´çªå‡º, ä¾‹å¦‚, ä½¿å…¶æ›´äº®æˆ–æ›´é”åˆ©. åœ¨æ•°å­—ä¸–ç•Œä¸­, é€šè¿‡ä½¿ç”¨é€‚åˆäºå›¾åƒè¯­ä¹‰çš„ç…§ç›¸æˆ–ç»˜ç”»æ»¤é•œå¤„ç†è‚–åƒå›¾åƒ, å¯ä»¥å®ç°ç±»ä¼¼çš„æ•ˆæœ. è™½ç„¶å­˜åœ¨è®¸å¤šæˆåŠŸçš„ç”¨æˆ·æŒ‡å¯¼æ–¹æ³•æ¥æç»˜è¯¥ä¸»é¢˜, ä½†ç¼ºä¹å…¨è‡ªåŠ¨æŠ€æœ¯å¹¶ä¸”äº§ç”Ÿä¸ä»¤äººæ»¡æ„çš„ç»“æœ. æˆ‘ä»¬çš„è®ºæ–‡é¦–å…ˆé€šè¿‡å¼•å…¥ä¸“ç”¨äºè‚–åƒçš„æ–°è‡ªåŠ¨åˆ†å‰²ç®—æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜. ç„¶å, æˆ‘ä»¬åœ¨æ­¤ç»“æœçš„åŸºç¡€ä¸Š, æè¿°äº†å‡ ä¸ªåˆ©ç”¨æˆ‘ä»¬çš„è‡ªåŠ¨åˆ†å‰²ç®—æ³•ç”Ÿæˆé«˜è´¨é‡è‚–åƒçš„è‚–åƒæ»¤é•œ.</p>
</blockquote>
<h3 id="deep-automatic-portrait-matting">Deep Automatic Portrait Matting</h3>
<p><img src="assets/2019-01-01-19-31-55.png" alt="img" /></p>
<ul>
<li>è®ºæ–‡: <a href="http://www.cse.cuhk.edu.hk/~leojia/projects/automatting/papers/deepmatting.pdf">http://www.cse.cuhk.edu.hk/~leojia/projects/automatting/papers/deepmatting.pdf</a></li>
<li>é¡¹ç›®:
<ul>
<li><a href="http://www.cse.cuhk.edu.hk/~leojia/projects/automatting/">http://www.cse.cuhk.edu.hk/~leojia/projects/automatting/</a></li>
<li><a href="http://xiaoyongshen.me/webpages/webpage_automatting/">http://xiaoyongshen.me/webpages/webpage_automatting/</a></li>
</ul></li>
<li>ä¸‹è½½:
<ul>
<li>[Data(zip, 1.15GB)] Please send Email to <a href="mailto:goodshenxy@gmail.com">goodshenxy@gmail.com</a> to request it.</li>
<li>ä½œè€…è‡ªå·±å…¬å¼€äº†: <a href="https://1drv.ms/u/s!ApwdOxIIFBH19Ts5EuFd9gVJrKTo">https://1drv.ms/u/s!ApwdOxIIFBH19Ts5EuFd9gVJrKTo</a></li>
</ul></li>
</ul>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºæ€§çŠ¶å›¾åƒçš„è‡ªåŠ¨å›¾åƒmattingæ–¹æ³•. è¯¥æ–¹æ³•ä¸éœ€è¦ç”¨æˆ·äº¤äº’, è¿™åœ¨å¤§å¤šæ•°å…ˆå‰çš„æ–¹æ³•ä¸­æ˜¯å¿…ä¸å¯å°‘çš„. ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡, æå‡ºäº†ä¸€ç§æ–°çš„ç«¯åˆ°ç«¯å·ç§¯ç¥ç»ç½‘ç»œ(CNN)æ¡†æ¶, å…¶é‡‡ç”¨è‚–åƒå›¾åƒçš„è¾“å…¥. å®ƒè¾“å‡ºmattingçš„ç»“æœ. æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…è€ƒè™‘å›¾åƒè¯­ä¹‰é¢„æµ‹, è¿˜è€ƒè™‘åƒç´ çº§å›¾åƒmatteä¼˜åŒ–. ä¸€ä¸ªæ–°çš„è‚–åƒimage datasetä¸æˆ‘ä»¬æ ‡è®°çš„mattingåŸºç¡€äº‹å®æ„æˆ. æˆ‘ä»¬çš„è‡ªåŠ¨æ–¹æ³•é€šè¿‡æœ€å…ˆè¿›çš„æ–¹æ³•è·å¾—äº†å¯æ¯”è¾ƒçš„ç»“æœ, è¯¥æ–¹æ³•éœ€è¦æŒ‡å®šçš„å‰æ™¯å’ŒèƒŒæ™¯åŒºåŸŸæˆ–åƒç´ .</p>
<p>æˆ‘ä»¬ä»Flickræ”¶é›†äº†è‚–åƒå›¾åƒ. ç„¶åé€‰æ‹©å®ƒä»¬ä»¥ç¡®ä¿è‚–åƒå…·æœ‰å„ç§å¹´é¾„, é¢œè‰², è¡£æœ, é…é¥°, å‘å‹, å¤´éƒ¨ä½ç½®, èƒŒæ™¯åœºæ™¯ç­‰.mattingåŒºåŸŸä¸»è¦æ˜¯ç”±äºæ™¯æ·±å¼•èµ·çš„å¤´å‘å’ŒæŸ”è½¯è¾¹ç¼˜. è£å‰ªæ‰€æœ‰å›¾åƒ, ä½¿å¾—é¢éƒ¨çŸ©å½¢å…·æœ‰ç›¸ä¼¼çš„å°ºå¯¸. é€šè¿‡é€‰å®šçš„è‚–åƒå›¾åƒ, æˆ‘ä»¬åˆ›å»ºäº†å…·æœ‰å¯†é›†ç”¨æˆ·äº¤äº’çš„alpha matte, ä»¥ç¡®ä¿å®ƒä»¬å…·æœ‰é«˜è´¨é‡.</p>
<p>é¦–å…ˆ, æˆ‘ä»¬æ ‡è®°æ¯ä¸ªå›¾åƒæ”¾å¤§åˆ°å±€éƒ¨åŒºåŸŸçš„ä¸‰å…ƒç»„.</p>
<p>ç„¶åæˆ‘ä»¬è®¡ç®—mattes, ä½¿ç”¨é—­å¼matting[1]å’ŒKNN matting[2].</p>
<p>æ¯ä¸ªå›¾åƒçš„ä¸¤ä¸ªè®¡ç®—é®ç½©è¦†ç›–èƒŒæ™¯å›¾åƒä»¥æ‰‹åŠ¨æ£€æŸ¥è´¨é‡. æˆ‘ä»¬ä¸ºæ•°æ®é›†é€‰æ‹©æ›´å¥½çš„ä¸€ä¸ª. å¦‚æœä¸¤ä¸ªmatteséƒ½ä¸ç¬¦åˆæˆ‘ä»¬çš„é«˜æ ‡å‡†, ç»“æœå°†è¢«ä¸¢å¼ƒ. å¿…è¦æ—¶, å°é”™è¯¯å¯ä»¥é€šè¿‡Photoshop[31]æ¥è§£å†³. åœ¨æ­¤æ ‡ç­¾å¤„ç†å, æˆ‘ä»¬æ”¶é›†äº†2, 000å¼ é«˜è´¨é‡é®ç½©å›¾åƒ. è¿™äº›å›¾åƒè¢«éšæœºåˆ†æˆè®­ç»ƒå’Œæµ‹è¯•é›†, åˆ†åˆ«å…·æœ‰1, 700å’Œ300ä¸ªå›¾åƒ.</p>
</blockquote>
<h2 id="other">Other</h2>
<h3 id="large-scale-fashion-deepfashion-database">Large-scale Fashion (DeepFashion) Database</h3>
<p><img src="assets/2019-03-22-18-57-36.png" alt="img" /></p>
<ul>
<li>é¡¹ç›®: <a href="http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html">http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html</a></li>
<li>ç»„ç»‡:
<ul>
<li><a href="http://mmlab.ie.cuhk.edu.hk/">Multimedia Laboratory</a></li>
<li><a href="http://www.cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong</a></li>
</ul></li>
<li>ä¸‹è½½: <a href="http://pan.baidu.com/s/1i43pnZR">http://pan.baidu.com/s/1i43pnZR</a> (æ›´å¤šç»†èŠ‚è¯·è§é¡¹ç›®ä¸»é¡µ)</li>
</ul>
<blockquote>
<p>æˆ‘ä»¬æä¾›DeepFashionæ•°æ®åº“, è¿™æ˜¯ä¸€ä¸ªå¤§å‹æœè£…æ•°æ®åº“, å®ƒæœ‰å‡ ä¸ªå¸å¼•äººçš„ç‰¹æ€§:</p>
<ul>
<li>é¦–å…ˆ, DeepFashionåŒ…å«è¶…è¿‡800, 000ç§ä¸åŒçš„æ—¶å°šå›¾åƒ, ä»ç²¾ç¾çš„å•†åº—å›¾åƒåˆ°æ— çº¦æŸçš„æ¶ˆè´¹è€…ç…§ç‰‡.</li>
<li>å…¶æ¬¡, DeepFashionæ³¨é‡Šäº†ä¸°å¯Œçš„æœè£…å•†å“ä¿¡æ¯. æ­¤æ•°æ®é›†ä¸­çš„æ¯ä¸ªå›¾åƒéƒ½æ ‡æœ‰50ä¸ªç±»åˆ«, 1, 000ä¸ªæè¿°æ€§å±æ€§, è¾¹ç•Œæ¡†å’Œæœè£…æ ‡è®°.</li>
<li>ç¬¬ä¸‰, DeepFashionåŒ…å«è¶…è¿‡300, 000ä¸ªäº¤å‰å§¿åŠ¿/è·¨åŸŸå›¾åƒå¯¹. ä½¿ç”¨DeepFashionæ•°æ®åº“å¼€å‘äº†å››ä¸ªåŸºå‡†, åŒ…æ‹¬å±æ€§é¢„æµ‹, æ¶ˆè´¹è€…åˆ°å•†åº—çš„è¡£æœæ£€ç´¢, åº—å†…è¡£æœæ£€ç´¢å’Œåœ°æ ‡æ£€æµ‹.</li>
</ul>
<p>è¿™äº›åŸºå‡†çš„æ•°æ®å’Œæ³¨é‡Šä¹Ÿå¯ä»¥ç”¨ä½œä»¥ä¸‹è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„è®­ç»ƒå’Œæµ‹è¯•é›†, ä¾‹å¦‚è¡£æœæ£€æµ‹, è¡£æœè¯†åˆ«å’Œå›¾åƒæ£€ç´¢. è¯·é˜…è¯»"ä¸‹è½½è¯´æ˜"ä»¥è®¿é—®æ•°æ®é›†.</p>
</blockquote>
<h3 id="ml-image">ML-Image</h3>
<ul>
<li>é¡¹ç›®: <a href="https://github.com/Tencent/tencent-ml-images#download-images-from-open-images">https://github.com/Tencent/tencent-ml-images#download-images-from-open-images</a></li>
</ul>
<blockquote>
<p>ML-Images: the largest open-source multi-label image database, including 17, 609, 752 training and 88, 739 validation image URLs, which are annotated with up to 11, 166 categories</p>
</blockquote>
<h2 id="need-your-help">need your help...</h2>
<blockquote>
<p>æœ‰äº›æ•°æ®é›†å·²ç»å¿˜è®°äº†å‡ºå¤„, å¤§å®¶æœ‰è§è¿‡çš„, å¸Œæœ›å¯ä»¥è¡¥å……ä¸‹.</p>
</blockquote>
<ul>
<li>Image Pair</li>
<li>Cosal2015</li>
<li>INCT2016</li>
<li>RGBDCoseg183</li>
<li>06RGBDCosal150</li>
<li>SegTrackV1/V2</li>
<li>ViSal</li>
<li>MCL</li>
<li>UVSD</li>
<li>VOS</li>
</ul>
<h2 id="reference">Reference</h2>
<h3 id="salient-object-detection-a-survey"><a href="https://arxiv.org/abs/1411.5878">Salient Object Detection: A Survey</a></h3>
<p><img src="./assets/2018-12-29-17-06-42.png" alt="img" /></p>
<p>è¯¦ç»†è¯„ä¼°: <a href="https://mmcheng.net/zh/salobjbenchmark/">https://mmcheng.net/zh/salobjbenchmark/</a> (è¿™é‡Œå±•ç¤ºäº†{THUR15K, JuddDB, DUT-OMRON, SED2, MSRA10K, ECSSD}å…­ç§æ•°æ®é›†çš„ä¸€ä¸ªæ¦œå•).</p>
<h3 id="review-of-visual-saliency-detection-with-comprehensive-information"><a href="https://arxiv.org/abs/1803.03391">Review of Visual Saliency Detection with Comprehensive Information</a></h3>
<p><img src="./assets/2018-12-27-11-05-49.png" alt="dataset" /></p>
<h3 id="salient-object-detection-in-the-deep-learning-era-an-in-depth-survey"><a href="https://www.researchgate.net/publication/332553805_Salient_Object_Detection_in_the_Deep_Learning_Era_An_In-Depth_Survey">Salient Object Detection in the Deep Learning Era: An In-Depth Survey</a></h3>
<ul>
<li>é¡¹ç›®: <a href="https://github.com/wenguanwang/SODsurvey">https://github.com/wenguanwang/SODsurvey</a></li>
<li>è¯´æ˜: æœ¬æ–‡æ¡£äº2019å¹´07æœˆ07æ—¥ä¿®æ”¹çš„å†…å®¹ä¸»è¦å‚è€ƒè‡ªè¯¥ç»¼è¿°è®ºæ–‡, æ„Ÿè°¢ä½œè€…çš„å·¥ä½œ, æ€»ç»“çš„éå¸¸è¯¦ç»†!</li>
</ul>
<h2 id="more">More</h2>
<h3 id="similiar-projects">Similiar Projects</h3>
<ul>
<li><a href="https://github.com/mrgloom/awesome-semantic-segmentation">awesome-semantic-segmentation</a></li>
</ul>
<h3 id="research-institutes">Research Institutes</h3>
<ul>
<li>ç™¾åº¦ç ”ç©¶é™¢: <a href="https://ai.baidu.com/broad/introduction">https://ai.baidu.com/broad/introduction</a></li>
<li>ä¸­å±±å¤§å­¦äººæœºç‰©æ™ºèƒ½èåˆå®éªŒå®¤: <a href="http://www.sysu-hcp.net/resources/">http://www.sysu-hcp.net/resources/</a></li>
<li>å¤§è¿ç†å·¥å¤§å­¦IIAU-LAB: <a href="http://ice.dlut.edu.cn/lu/publications.html">http://ice.dlut.edu.cn/lu/publications.html</a></li>
<li>CUHK Multimedia Laboratory: <a href="http://mmlab.ie.cuhk.edu.hk/datasets.html">http://mmlab.ie.cuhk.edu.hk/datasets.html</a></li>
</ul>
<h3 id="resource-websites">Resource Websites</h3>
<ul>
<li>TC-11 Online Resources: <a href="http://tc11.cvc.uab.es/datasets/type/">http://tc11.cvc.uab.es/datasets/type/</a></li>
<li>CVonline: Image Databases: <a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm">http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm</a>
<ul>
<li>ä¸­æ–‡: <a href="https://blog.csdn.net/zhaoliang027/article/details/83376167">https://blog.csdn.net/zhaoliang027/article/details/83376167</a></li>
</ul></li>
<li>MediaEval Benchmark: <a href="http://www.multimediaeval.org/datasets/">http://www.multimediaeval.org/datasets/</a></li>
<li>Mit Saliency Benchmark: <a href="http://saliency.mit.edu/datasets.html">http://saliency.mit.edu/datasets.html</a></li>
<li>Datasets for machine learning: <a href="https://www.datasetlist.com/">https://www.datasetlist.com/</a></li>
<li>UCI machine learning repository:<a href="https://archive.ics.uci.edu/ml/datasets.html">https://archive.ics.uci.edu/ml/datasets.html</a></li>
<li>Kaggle datasets:<a href="https://www.kaggle.com/datasets">https://www.kaggle.com/datasets</a></li>
<li>Google
<ul>
<li>Dataset Seaerch: <a href="https://toolbox.google.com/datasetsearch">https://toolbox.google.com/datasetsearch</a></li>
<li><a href="https://ai.google/tools/datasets/">https://ai.google/tools/datasets/</a></li>
</ul></li>
</ul>
<h2 id="about">About</h2>
<ul>
<li>Edited by Lart Pang</li>
<li>Tools: VSCode</li>
<li>Plugins:
<ul>
<li>Markdown All in One</li>
<li>markdown-formatter(éšç€ä¸æ–­åœ°æäº†ä¸€äº›issue(<a href="https://github.com/sumnow/markdown-formatter/issues/5">#5</a>, <a href="https://github.com/sumnow/markdown-formatter/issues/6">#6</a>, <a href="https://github.com/sumnow/markdown-formatter/issues/7">#7</a>, <a href="https://github.com/sumnow/markdown-formatter/issues/8">#8</a>, <a href="https://github.com/sumnow/markdown-formatter/issues/9">#9</a>), è¶Šæ¥è¶Šå¥½ç”¨äº†, å¼ºçƒˆæ¨è)</li>
<li>Paste Image</li>
</ul></li>
</ul>
</body>
</html>
